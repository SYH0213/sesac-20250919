# =================================================================================================
# íŒŒì¼ ì„¤ëª…: CRAGv6.py
#
# ê¸°ëŠ¥:
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PDF ë¬¸ì„œ ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡ì„ Gradio ì›¹ UIë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.
# LangChainì˜ Agent ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, LLMì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì—¬ 'ë„êµ¬(Tool)'ë¥¼ ë™ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”
# ê³ ê¸‰ RAG(Retrieval-Augmented Generation) ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
#
# ì£¼ìš” ì•„í‚¤í…ì²˜ ë° ê¸°ëŠ¥:
# 1.  **ì—ì´ì „íŠ¸ (Agent) ê¸°ë°˜ ë™ì  íŒŒì´í”„ë¼ì¸**:
#     - ê³ ì •ëœ ìˆœì„œ(LangGraph) ëŒ€ì‹ , LLMì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ìŠ¤ìŠ¤ë¡œ ì„ íƒí•˜ê³  í˜¸ì¶œí•©ë‹ˆë‹¤.
#     - ì´ë¥¼ í†µí•´ "PDF ë‚´ìš©ê³¼ ìµœì‹  ë‰´ìŠ¤ë¥¼ ë¹„êµí•´ì¤˜"ì™€ ê°™ì€ ë³µí•©ì ì¸ ì§ˆë¬¸ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# 2.  **ë„êµ¬ (Tools)**:
#     - **PDF ê²€ìƒ‰**: `history_aware_retriever`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
#     - **ì›¹ ê²€ìƒ‰**: `TavilySearch`ë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì— ì—†ëŠ” ìµœì‹  ì •ë³´ë‚˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ì›¹ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•©ë‹ˆë‹¤.
#
# 3.  **ë¬¸ì„œ ì²˜ë¦¬ ë° RAG**:
#     - `gemini_parser` (ë˜ëŠ” LlamaParse)ë¥¼ ì‚¬ìš©í•´ PDF ë¬¸ì„œë¥¼ Markdownìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#     - `ParentDocumentRetriever`ì™€ `ChromaDB`ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë¬¸ì„œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#
# 4.  **ë©”ëª¨ë¦¬ ê´€ë¦¬**:
#     - Stateless êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©°, ë§¤ ìš”ì²­ë§ˆë‹¤ UIë¡œë¶€í„° ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ë°›ì•„ ì—ì´ì „íŠ¸ì˜ íŒë‹¨ì— í™œìš©í•©ë‹ˆë‹¤.
#
# 5.  **UI**:
#     - Gradioë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì‰½ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
# =================================================================================================

import os
import json
import traceback
import gradio as gr
from dotenv import load_dotenv
from gemini_parser import parse_pdf_to_markdown

# Python typing
from typing import Iterable, Optional, Tuple, List, Any
from typing_extensions import TypedDict

# LangChain - Core
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.stores import BaseStore
from langchain.schema import Document, StrOutputParser

# LangChain - LLMs & Embeddings
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# LangChain - Document Processing & Retrieval
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.retrievers import ParentDocumentRetriever
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# LangChain - Agent & Tools
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.tools import Tool
from langchain_tavily import TavilySearch


# --------------------------
# ìœ í‹¸: Gradioìš© íˆìŠ¤í† ë¦¬ ë³€í™˜
# --------------------------
def to_lc_messages(history: List[dict]) -> List[BaseMessage]:
    msgs: List[BaseMessage] = []
    for m in history:
        if m["role"] == "user":
            msgs.append(HumanMessage(content=m["content"]))
        elif m["role"] == "assistant":
            msgs.append(AIMessage(content=m["content"]))
    return msgs

def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:
    history = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            history.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            history.append({"role": "assistant", "content": msg.content})
    return history

# --------------------------
# ì „ì—­ Debug Log ì €ì¥ì†Œ
# --------------------------
debug_logs = []

def log_debug(msg: str):
    debug_logs.append(msg)
    print(msg)

# --------------------------
# íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore
# --------------------------
class JSONDocStore(BaseStore[str, Document]):
    def __init__(self, root_dir: str = "./parent_store"):
        self.root_dir = root_dir
        os.makedirs(self.root_dir, exist_ok=True)

    def _path(self, key: str) -> str:
        return os.path.join(self.root_dir, f"{key}.json")

    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:
        for key, doc in key_value_pairs:
            with open(self._path(key), "w", encoding="utf-8") as f:
                json.dump({"page_content": doc.page_content, "metadata": doc.metadata}, f, ensure_ascii=False)

    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:
        results: List[Optional[Document]] = []
        for key in keys:
            p = self._path(key)
            if os.path.exists(p):
                with open(p, "r", encoding="utf-8") as f:
                    data = json.load(f)
                results.append(Document(page_content=data.get("page_content", ""), metadata=data.get("metadata", {})))
            else:
                results.append(None)
        return results

    def mdelete(self, keys: Iterable[str]) -> None:
        for key in keys:
            p = self._path(key)
            if os.path.exists(p):
                os.remove(p)

    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:
        for fname in os.listdir(self.root_dir):
            if fname.endswith(".json"):
                key = fname[:-5]
                if prefix is None or key.startswith(prefix):
                    yield key

# --------------------------
# í™˜ê²½ë³€ìˆ˜ ë° ì„¤ì •
# --------------------------
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
LLAMA_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
TAVILY_KEY = os.getenv("TAVILY_API_KEY")

PDF_NAME = "gemini-2.5-tech_1-3"
PDF_PATH = f"data/{PDF_NAME}.pdf"
PARSED_MD_PATH = f"loaddata/gemini_parsed_{PDF_NAME}.md"
CHROMA_DB_DIR = "./chroma_db3"

# --------------------------
# ëª¨ë¸ ë° ì„ë² ë”©
# --------------------------
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# --------------------------
# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ
# --------------------------
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)
store = JSONDocStore("./parent_store3")

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
    search_kwargs={"k": 2},
)

def _vs_count_safe() -> int:
    try:
        return vectorstore._collection.count()
    except Exception:
        return 0

def load_and_populate_vectorstore():
    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    if _vs_count_safe() > 0:
        print(f"[INFO] Vector store already populated. Count={_vs_count_safe()}")
        return

    try:
        markdown_file_path = parse_pdf_to_markdown(PDF_PATH, output_dir=os.path.dirname(PARSED_MD_PATH))
    except Exception as e:
        raise RuntimeError(f"Gemini Parserë¥¼ ì‚¬ìš©í•œ PDF íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    print(f"[INFO] Loading markdown from '{markdown_file_path}'...")
    with open(markdown_file_path, "r", encoding="utf-8") as f:
        text = f.read()
    documents = [Document(page_content=text, metadata={"source": PARSED_MD_PATH})]
    retriever.add_documents(documents)
    print(f"[INFO] Vector store populated. Count={_vs_count_safe()}")

# --------------------------
# ì—ì´ì „íŠ¸ ë„êµ¬(Tools) ì •ì˜
# --------------------------

# --- Tool 1: PDF ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ---
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood without the chat history. "
    "Do NOT answer the question, just reformulate it if needed and otherwise return it as is."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
])
history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

qa_system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer the question. "
    "If you don't know the answer, just say that you don't know. "
    "Keep the answer concise and answer in Korean."
    "\n\n{context}"
)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
])
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# RAG Chainì„ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ë˜í¼ í•¨ìˆ˜
# ì´ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ì˜ ë„êµ¬ê°€ í˜¸ì¶œë  ë•Œ í˜„ì¬ ëŒ€í™” ê¸°ë¡ì„ ì˜¬ë°”ë¥´ê²Œ ì°¸ì¡°í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.
def run_rag_chain_with_history(input_dict: dict) -> str:
    # run_agent í•¨ìˆ˜ì—ì„œ ì„¤ì •ëœ ì „ì—­ ëŒ€í™” ê¸°ë¡ì„ ì‚¬ìš©
    chat_history = to_lc_messages(current_history_for_tool)
    log_debug(f"---PDF ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰--- (ì…ë ¥: {input_dict.get('input')})")
    result = rag_chain.invoke({"input": input_dict.get("input"), "chat_history": chat_history})
    log_debug(f"---PDF ê²€ìƒ‰ ë„êµ¬ ì™„ë£Œ--- (ë‹µë³€: {result.get('answer')})")
    return result.get("answer", "PDFì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# --- Tool 2: ì›¹ ê²€ìƒ‰ ë„êµ¬ ---
web_search_tool = TavilySearch(k=3) if TAVILY_KEY else None

# --- ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ìƒì„± ---
tools = []
tools.append(Tool(
    name="pdf_search",
    func=lambda q: run_rag_chain_with_history({"input": q}),
    description="PDF ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ PDF ë¬¸ì„œì— ëŒ€í•œ ê²ƒì¼ ê²½ìš° ì´ ë„êµ¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.",
))
if web_search_tool:
    tools.append(Tool(
        name="web_search",
        func=web_search_tool.invoke,
        description="ìµœì‹  ì •ë³´ë‚˜ PDF ë¬¸ì„œì— í¬í•¨ë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì£¼ì œì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
    ))
else:
    print("[WARN] TAVILY_API_KEYê°€ ì—†ì–´ ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")

# --------------------------
# ì—ì´ì „íŠ¸(Agent) ìƒì„±
# --------------------------
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. You must answer in Korean. "
               "Analyze the user's question and the conversation history, "
               "and decide whether to use a tool or answer directly. "
               "If the question is about the content of the PDF, use the 'pdf_search' tool. "
               "If the question is about a general topic or requires recent information, use the 'web_search' tool. "
               "For simple greetings or conversational remarks, answer directly without using tools."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

agent = create_openai_tools_agent(llm, tools, agent_prompt)
# verbose=Trueë¡œ ì„¤ì •í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ìƒê° ê³¼ì •ì„ ì½˜ì†”ì— ì¶œë ¥
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# --------------------------
# Gradio ì‹¤í–‰ í•¨ìˆ˜
# --------------------------
# ë„êµ¬ê°€ í˜„ì¬ ëŒ€í™” ê¸°ë¡ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
current_history_for_tool = []

def run_agent(query: str, history: List[dict], show_debug: bool):
    global debug_logs, current_history_for_tool
    debug_logs = []
    # ì—ì´ì „íŠ¸ ì‹¤í–‰ ì „ì— í˜„ì¬ íˆìŠ¤í† ë¦¬ë¥¼ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •
    current_history_for_tool = history or []

    # Gradioì˜ íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    chat_history_for_agent = to_lc_messages(current_history_for_tool)

    try:
        log_debug("---ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘---")
        # ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œ, ì½˜ì†”ì— verbose ì¶œë ¥ì´ ë‚˜íƒ€ë‚¨
        response = agent_executor.invoke({
            "input": query,
            "chat_history": chat_history_for_agent
        })
        answer = response.get("output", "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        log_debug(f"---ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ--- (ìµœì¢… ë‹µë³€: {answer})")

        # Gradio UIì— í‘œì‹œí•  íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        new_history = current_history_for_tool + [{"role": "user", "content": query}, {"role": "assistant", "content": answer}]

        # ë””ë²„ê·¸ ì •ë³´ (AgentExecutorì˜ verbose ì¶œë ¥ì´ ì½˜ì†”ì— ì°íˆë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í‘œì‹œ)
        context_md = "ì—ì´ì „íŠ¸ê°€ ë™ì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ íŒë‹¨í•˜ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ê³¼ì •ì€ ì‹¤í–‰ ì½˜ì†”ì˜ `[agent_executor]` ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        debug_output = "### Debug Logs\n```\n" + "\n".join(debug_logs) + "\n```" if show_debug else ""

        return "", new_history, context_md, debug_output

    except Exception as e:
        err = f"ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}"
        log_debug(err)
        debug_output = "### ì˜¤ë¥˜\n```\n" + err + "\n```"
        error_history = current_history_for_tool + [{"role": "user", "content": query}, {"role": "assistant", "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}]
        return "", error_history, "ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.", debug_output

def force_reload_vectorstore():
    try:
        print("[INFO] Resetting Chroma client...")
        vectorstore._client.reset()
        load_and_populate_vectorstore()
        return "âœ… Vector store reloaded successfully!"
    except Exception as e:
        return f"âŒ Error during vector store reload: {e}"

# --------------------------
# ì´ˆê¸°í™” ë° UI ì‹¤í–‰
# --------------------------
load_and_populate_vectorstore()

example_questions = [
    "Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?",
    "Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?",
    "ì˜¤ëŠ˜ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ ë‚ ì”¨ëŠ” ì–´ë•Œ?",
    "ì•ˆë…•? ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?",
]

with gr.Blocks(theme="soft", title="Agent-based PDF Chatbot") as demo:
    gr.Markdown("# Agent-based PDF Chatbot (Dynamic Tool Use)")
    gr.Markdown("ì—ì´ì „íŠ¸ê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ PDF ê²€ìƒ‰, ì›¹ ê²€ìƒ‰, ë˜ëŠ” ì§ì ‘ ë‹µë³€ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.")

    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=450, label="Chat", type="messages", value=[])
            msg = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)")
            gr.Examples(examples=example_questions, inputs=msg, label="ì˜ˆì‹œ ì§ˆë¬¸")

        with gr.Column(scale=1):
            context_display = gr.Markdown(label="Agent Context / Debug")
            with gr.Accordion("âš™ï¸ Advanced Options", open=False):
                show_debug_checkbox = gr.Checkbox(label="Show Debug Logs in UI", value=False)
                reload_button = gr.Button("ğŸ”„ Force Reload Vector Store")
                reload_status = gr.Markdown()

    clear = gr.ClearButton([msg, chatbot, context_display])
    msg.submit(run_agent, [msg, chatbot, show_debug_checkbox], [msg, chatbot, context_display, context_display])
    reload_button.click(force_reload_vectorstore, outputs=reload_status)

demo.launch()
