{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e598d47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08f1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# --- Document Loading and Caching ---\n",
    "PDF_PATH = \"data/gemini-2.5-tech_3.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_3.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ea157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Splitters\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# 2. Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3. Vectorstore\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# 4. ParentDocumentRetriever\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\":2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852c1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_populate_vectorstore():\n",
    "    if vectorstore._collection.count() > 0:\n",
    "        return\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        parser = LlamaParse(result_type=\"markdown\", api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"))\n",
    "        documents = parser.load_data(PDF_PATH)\n",
    "        with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join([doc.text for doc in documents]))\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    retriever.add_documents([Document(page_content=text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851f6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prompt for History-Aware Retriever\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b6900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the History-Aware Retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de30e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prompt for Final Answer Generation\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your ONLY task is to answer the user's question STRICTLY based on the provided context. \"\n",
    "    \"If the information to answer the question is present in the context, provide a concise answer. \"\n",
    "    \"If the answer cannot be found within the provided context, you MUST say '제공된 문서의 내용으로는 답변할 수 없습니다.' Do NOT use any of your outside knowledge.\"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the Document Chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --- RAG Function ---\n",
    "def ask_llm(query, history):\n",
    "    chat_history_for_chain = []\n",
    "    if history:\n",
    "        for message in history:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                chat_history_for_chain.append(HumanMessage(content=message[\"content\"]))\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                chat_history_for_chain.append(AIMessage(content=message[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d505f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00425235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1914: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23071e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fc09a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is agent memory and how does it function in artificial intelligence?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question = \"agent memory\"\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e570497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "# --- GraphState 정의 ---\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    chat_history: list\n",
    "\n",
    "# --- Node 함수들 ---\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # history-aware retriever 사용\n",
    "    retrieved_docs = history_aware_retriever.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return {\"documents\": retrieved_docs, \"question\": question, \"chat_history\": chat_history}\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        if score.binary_score == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": answer, \"chat_history\": chat_history}\n",
    "\n",
    "def transform_query(state):\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": state[\"documents\"], \"question\": better_question, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def web_search_node(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    documents = state[\"documents\"]\n",
    "    documents.append(Document(page_content=web_results))\n",
    "    return {\"documents\": documents, \"question\": question, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    if state[\"web_search\"] == \"Yes\":\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "\n",
    "# --- 그래프 구성 ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search_node)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\"transform_query\": \"transform_query\", \"generate\": \"generate\"}\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6ec19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "{'retrieve': {'documents': [], 'question': 'Gemini 2.5 Pro는 무엇이 달라졌나요?', 'chat_history': []}}\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "{'grade_documents': {'documents': [], 'question': 'Gemini 2.5 Pro는 무엇이 달라졌나요?', 'web_search': 'No', 'chat_history': []}}\n",
      "---GENERATE---\n",
      "{'generate': {'documents': [], 'question': 'Gemini 2.5 Pro는 무엇이 달라졌나요?', 'generation': '제공된 문서의 내용으로는 답변할 수 없습니다.', 'chat_history': []}}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Gemini 2.5 Pro는 무엇이 달라졌나요?\", \"chat_history\": []}\n",
    "for output in app.stream(inputs):\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a7d9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "382bada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) 통합 버전 (Gradio UI)\n",
    "# - PDF → LlamaParse(md) → Chroma(Parent/Child) → History-Aware Retrieve\n",
    "# - CRAG: grade_documents → (generate | transform_query → web_search → generate)\n",
    "# - 문서 외 지식 금지, 없으면 한국어로 \"제공된 문서...\" 출력\n",
    "# - 웹검색: Tavily(선택, 미설정 시 우회)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # ✅ BaseStore는 여기로 이동됨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) 웹 검색 툴\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# 유틸: Gradio용 히스토리 변환\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 전역 Debug Log 저장소\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # 콘솔에도 그대로 찍어줌\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    간단한 파일 기반 영구 DocStore.\n",
    "    - key -> ./parent_store/{key}.json 에 Document 저장\n",
    "    - ParentDocumentRetriever 가 요구하는 mset/mget/mdelete/yield_keys 구현\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 환경변수 로드 & 설정\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # 없으면 웹검색 보강은 건너뜀\n",
    "\n",
    "# --------------------------\n",
    "# 경로 및 전역 설정\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & 임베딩\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# 기존: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # 파일 기반 parent 저장\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 데이터 로딩 & 벡터DB 적재\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # 내부 속성 의존을 최소화하는 안전한 카운트 함수\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma 내부\n",
    "    except Exception:\n",
    "        try:\n",
    "            # 간단히 비슷문서 조회 시도 (비어있으면 예외 or 빈 결과)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # Chroma는 비어있어도 호출이 성공할 수 있으므로 peek 써봄\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD 파일 없으면 PDF → LlamaParse → md 저장\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEY가 없어 PDF 파싱을 수행할 수 없습니다.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse 오류: {e}\")\n",
    "\n",
    "    # md 로드 → Parent retriever에 추가\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 하나의 거대 문서로 추가 → Parent/Child splitter가 내부에서 잘게 쪼갬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# 최종 답변(문서 기반만 허용) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 문서 관련성 평가(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 질문 재작성 (웹검색 친화적)\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (선택) 웹검색 도구\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY 미설정: 웹검색 보강은 생략됩니다.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 상태 정의\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]  # 항상 HumanMessage/AIMessage만\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 노드 함수\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # 원 질문 + 히스토리 출력\n",
    "    log_debug(f\"[DEBUG] Raw Question: {question}\")\n",
    "    if chat_history:\n",
    "        log_debug(f\"[DEBUG] Chat History Count: {len(chat_history)}\")\n",
    "    else:\n",
    "        log_debug(\"[DEBUG] No chat history provided.\")\n",
    "\n",
    "    # Child 검색 결과 확인\n",
    "    child_results = vectorstore.similarity_search(question, k=2)\n",
    "    log_debug(\"=== Child 검색 결과 ===\")\n",
    "    for i, d in enumerate(child_results, 1):\n",
    "        log_debug(f\"[Child {i}] Parent ID: {d.metadata.get('doc_id')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:200]}...\\n\")\n",
    "\n",
    "    # Parent 복구 결과 (History-aware retriever 사용)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(\"=== Parent 복구 결과 ===\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        log_debug(f\"[Parent {i}] Source: {d.metadata.get('source', 'N/A')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:500]}...\\n\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # 그레이더 실패 시 일단 보수적으로 유지\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        # 웹검색 불가 시 안내 문서 추가\n",
    "        web_results_text = \"웹검색 API 키가 설정되지 않아 웹검색을 수행하지 못했습니다.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "            lines = []\n",
    "            for r in results:\n",
    "                # r keys: content, url, score, title 등\n",
    "                title = r.get(\"title\") or \"\"\n",
    "                url = r.get(\"url\") or \"\"\n",
    "                content = r.get(\"content\") or \"\"\n",
    "                lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "            web_results_text = \"\\n---\\n\".join(lines) if lines else \"검색결과가 비어 있습니다.\"\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"웹검색 중 오류: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # 답변 생성\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # 출처 구분\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[출처: 웹검색 결과]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[출처: PDF 문서]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 새 노드: 사용자 알림\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"문서에서 답변을 찾지 못했습니다. 인터넷 검색을 시도합니다.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # ✅ dict로만 유지\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 그래프 구성 & 컴파일\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)   # ✅ 추가\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 조건 분기 수정\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",            # 문서 관련성 O → 바로 답변\n",
    "        \"notify_user\": \"notify_user\",  # 문서 관련성 X → 먼저 notify_user\n",
    "    },\n",
    ")\n",
    "\n",
    "# 알림 후 웹검색 분기 연결\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# run_crag 수정\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict], show_debug: bool):\n",
    "    global debug_logs\n",
    "    debug_logs = []  # 실행할 때마다 초기화\n",
    "\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain,\n",
    "                  \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            for node_name, node_state in step.items():\n",
    "                log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state\n",
    "\n",
    "        # 최종 응답\n",
    "        answer = final_state.get(\"generation\", \"제공된 문서의 내용으로는 답변할 수 없습니다.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## 참조 문서\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### 문서 {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"참조된 문서가 없습니다.\"\n",
    "\n",
    "        # 히스토리 추가\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        # 디버그 표시 여부 결정\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        return \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"오류 발생: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### 오류\\n```\\n\" + err + \"\\n```\"\n",
    "        return \"\", history, \"참조된 문서가 없습니다.\", debug_output\n",
    "\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # 전체 컬렉션 초기화\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"✅ Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 초기 적재\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 Pro는 Gemini 1.5 Pro와 비교했을 때 어떤 점에서 향상되었나요?\",\n",
    "    \"Gemini 2.5 Pro와 Flash는 어떤 종류의 데이터를 처리할 수 있나요?\",\n",
    "    \"Gemini 2.5 시리즈의 작은 모델들은 어떤 방식으로 성능을 개선했나요?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF 문서 내용에 대해 질문하세요. 문서에서 못 찾으면 질문 재작성 + (선택)웹검색으로 보강합니다.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ------------------------------\n",
    "        # 왼쪽: 채팅 영역\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요... (Shift+Enter 줄바꿈)\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"예시 질문\"\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        # 오른쪽: 문서/옵션/디버그 영역\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM 참조 문서 전문/요약\")\n",
    "\n",
    "            with gr.Accordion(\"⚙️ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")   # ✅ 디버그 로그 출력 패널\n",
    "                reload_button = gr.Button(\"🔄 Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    # ------------------------------\n",
    "    # 버튼/이벤트 바인딩\n",
    "    # ------------------------------\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel])\n",
    "    msg.submit(run_crag, [msg, chatbot, show_debug_checkbox],\n",
    "               [msg, chatbot, context_display, debug_panel])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f99de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vector store already populated. Count=174\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) 통합 버전 (Gradio UI)\n",
    "# - PDF → LlamaParse(md) → Chroma(Parent/Child) → History-Aware Retrieve\n",
    "# - CRAG: grade_documents → (generate | transform_query → web_search → generate)\n",
    "# - 문서 외 지식 금지, 없으면 한국어로 \"제공된 문서...\" 출력\n",
    "# - 웹검색: Tavily(선택, 미설정 시 우회)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # ✅ BaseStore는 여기로 이동됨\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) 웹 검색 툴\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    간단한 파일 기반 영구 DocStore.\n",
    "    - key -> ./parent_store/{key}.json 에 Document 저장\n",
    "    - ParentDocumentRetriever 가 요구하는 mset/mget/mdelete/yield_keys 구현\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 환경변수 로드 & 설정\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # 없으면 웹검색 보강은 건너뜀\n",
    "\n",
    "# --------------------------\n",
    "# 경로 및 전역 설정\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & 임베딩\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# 기존: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # 파일 기반 parent 저장\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 데이터 로딩 & 벡터DB 적재\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # 내부 속성 의존을 최소화하는 안전한 카운트 함수\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma 내부\n",
    "    except Exception:\n",
    "        try:\n",
    "            # 간단히 비슷문서 조회 시도 (비어있으면 예외 or 빈 결과)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # Chroma는 비어있어도 호출이 성공할 수 있으므로 peek 써봄\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD 파일 없으면 PDF → LlamaParse → md 저장\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEY가 없어 PDF 파싱을 수행할 수 없습니다.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse 오류: {e}\")\n",
    "\n",
    "    # md 로드 → Parent retriever에 추가\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 하나의 거대 문서로 추가 → Parent/Child splitter가 내부에서 잘게 쪼갬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# 최종 답변(문서 기반만 허용) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"If the answer cannot be found within the provided context, you MUST say '제공된 문서나 검색 결과의 내용으로는 답변할 수 없습니다.' \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 문서 관련성 평가(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 질문 재작성 (웹검색 친화적)\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (선택) 웹검색 도구\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY 미설정: 웹검색 보강은 생략됩니다.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 상태 정의\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List  # HumanMessage/AIMessage 리스트\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 노드 함수\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    docs = history_aware_retriever.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "    return {\"documents\": docs, \"question\": question, \"chat_history\": chat_history, \"web_search\": \"No\", \"generation\": \"\"}\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # 그레이더 실패 시 일단 보수적으로 유지\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"transform_query\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        # 웹검색 불가 시 안내 문서 추가\n",
    "        web_results_text = \"웹검색 API 키가 설정되지 않아 웹검색을 수행하지 못했습니다.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "            lines = []\n",
    "            for r in results:\n",
    "                # r keys: content, url, score, title 등\n",
    "                title = r.get(\"title\") or \"\"\n",
    "                url = r.get(\"url\") or \"\"\n",
    "                content = r.get(\"content\") or \"\"\n",
    "                lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "            web_results_text = \"\\n---\\n\".join(lines) if lines else \"검색결과가 비어 있습니다.\"\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"웹검색 중 오류: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # 답변 생성\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # 출처 구분\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[출처: 웹검색 결과]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[출처: PDF 문서]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 그래프 구성 & 컴파일\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\"transform_query\": \"transform_query\", \"generate\": \"generate\"},\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 유틸: Gradio용 히스토리 변환\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio 핸들러\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict]):\n",
    "    # 1) Chat history 변환\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "\n",
    "    # 2) 그래프 실행\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain, \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            # 디버깅 로그(선택)\n",
    "            for node_name, node_state in step.items():\n",
    "                print(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state  # 마지막 state\n",
    "\n",
    "        # 3) 최종 응답/문서 정리\n",
    "        answer = final_state.get(\"generation\", \"제공된 문서의 내용으로는 답변할 수 없습니다.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## 참조 문서\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:800] + (\"...\" if len(d.page_content) > 800 else \"\")\n",
    "                context_md += f\"### 문서 {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"참조된 문서가 없습니다.\"\n",
    "\n",
    "        # 4) 히스토리에 추가\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        return \"\", history, context_md\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"오류 발생: {e}\\n{traceback.format_exc()}\"\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"오류가 발생했습니다. 콘솔 로그를 확인하세요.\"})\n",
    "        return \"\", history, f\"### 오류\\n```\\n{err}\\n```\"\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # 전체 컬렉션 초기화\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"✅ Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 초기 적재\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 Pro는 Gemini 1.5 Pro와 비교했을 때 어떤 점에서 향상되었나요?\",\n",
    "    \"Gemini 2.5 Pro와 Flash는 어떤 종류의 데이터를 처리할 수 있나요?\",\n",
    "    \"Gemini 2.5 시리즈의 작은 모델들은 어떤 방식으로 성능을 개선했나요?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF 문서 내용에 대해 질문하세요. 문서에서 못 찾으면 질문 재작성 + (선택)웹검색으로 보강합니다.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요... (Shift+Enter 줄바꿈)\")\n",
    "\n",
    "            gr.Examples(examples=example_questions, inputs=msg, label=\"예시 질문\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            context_display = gr.Markdown(label=\"LLM 참조 문서 전문/요약\")\n",
    "            with gr.Accordion(\"⚙️ Advanced Options\", open=False):\n",
    "                reload_button = gr.Button(\"🔄 Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display])\n",
    "    msg.submit(run_crag, [msg, chatbot], [msg, chatbot, context_display])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8d3ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================================\n",
    "# 파일 설명: CRAGv4.py\n",
    "#\n",
    "# 기능:\n",
    "# 이 스크립트는 PDF 문서 기반의 질의응답 챗봇을 Gradio 웹 UI로 구현한 것입니다.\n",
    "# LangChain과 LangGraph를 기반으로 Corrective RAG (CRAG) 및 일반 RAG 파이프라인을 통합하여,\n",
    "# 사용자의 질문 의도에 따라 동적으로 응답 방식을 변경하는 고급 기능을 포함합니다.\n",
    "#\n",
    "# 주요 아키텍처 및 기능:\n",
    "# 1.  **문서 처리**:\n",
    "#     - LlamaParse를 사용해 PDF 문서를 Markdown 형식으로 변환하여 텍스트와 테이블 구조를 정확하게 추출합니다.\n",
    "#     - 처리된 Markdown은 이후 실행 시 재사용을 위해 캐싱됩니다.\n",
    "#\n",
    "# 2.  **RAG (Retrieval-Augmented Generation)**:\n",
    "#     - `ParentDocumentRetriever`: 문서를 부모/자식 청크로 분할하여 검색 정확도와 컨텍스트 유지의 균형을 맞춥니다.\n",
    "#     - `ChromaDB`: 벡터 저장소로 사용되며, 임베딩된 문서 청크를 영구적으로 저장합니다.\n",
    "#     - `History-Aware Retriever`: 대화의 맥락을 이해하여 후속 질문(예: \"그건 어때?\")을 독립적인 질문으로 재구성합니다.\n",
    "#\n",
    "# 3.  **CRAG (Corrective RAG) & 라우팅 (LangGraph 기반)**:\n",
    "#     - **의도 분류 (Intent Classification)**: 사용자의 입력을 '단순 대화'와 '정보성 질문'으로 분류하는 라우팅 노드를 가장 먼저 실행합니다.\n",
    "#     - **대화형 응답**: '단순 대화'로 분류되면, RAG 파이프라인을 건너뛰고 LLM이 직접 대화형 답변을 생성합니다.\n",
    "#     - **문서 관련성 평가**: '정보성 질문'의 경우, 검색된 문서가 질문과 관련이 있는지 LLM이 평가합니다.\n",
    "#     - **웹 검색 보강**: 관련 문서가 없다고 판단되면, 질문을 웹 검색에 더 적합하게 변형한 후 Tavily API를 통해 웹 검색을 수행하고, 그 결과를 바탕으로 답변을 생성합니다.\n",
    "#\n",
    "# 4.  **메모리 관리**:\n",
    "#     - Stateless 구조: 서버는 대화 기록을 저장하지 않으며, 매 요청마다 Gradio UI(브라우저)로부터 전체 대화 기록을 전달받아 사용합니다.\n",
    "#     - UI 알림 버그 수정: 웹 검색 시 '인터넷 검색을 시도합니다'와 같은 중간 과정의 알림이 UI에 정상적으로 표시되도록 `run_crag` 함수의 히스토리 관리 로직을 수정했습니다.\n",
    "#\n",
    "# 5.  **UI**:\n",
    "#     - Gradio를 사용하여 사용자가 쉽게 상호작용할 수 있는 채팅 인터페이스를 제공합니다.\n",
    "# =================================================================================================\n",
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) 통합 버전 (Gradio UI)\n",
    "# - PDF → LlamaParse(md) → Chroma(Parent/Child) → History-Aware Retrieve\n",
    "# - CRAG: grade_documents → (generate | transform_query → web_search → generate)\n",
    "# - 문서 외 지식 금지, 없으면 한국어로 \"제공된 문서...\" 출력\n",
    "# - 웹검색: Tavily(선택, 미설정 시 우회)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from gemini_parser import parse_pdf_to_markdown\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # ✅ BaseStore는 여기로 이동됨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) 웹 검색 툴\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# 유틸: Gradio용 히스토리 변환\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:\n",
    "    history = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 전역 Debug Log 저장소\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # 콘솔에도 그대로 찍어줌\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    간단한 파일 기반 영구 DocStore.\n",
    "    - key -> ./parent_store/{key}.json 에 Document 저장\n",
    "    - ParentDocumentRetriever 가 요구하는 mset/mget/mdelete/yield_keys 구현\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 환경변수 로드 & 설정\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # 없으면 웹검색 보강은 건너뜀\n",
    "\n",
    "# --------------------------\n",
    "# 경로 및 전역 설정\n",
    "# --------------------------\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# LLM & 임베딩\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------------------\n",
    "# 동적 경로 관리 헬퍼\n",
    "# --------------------------------------\n",
    "def get_paths_for_pdf(pdf_filename: str):\n",
    "    \"\"\"선택된 PDF 파일명에 따라 동적으로 경로들을 생성합니다.\"\"\"\n",
    "    if not pdf_filename:\n",
    "        return None\n",
    "    \n",
    "    base_name = pathlib.Path(pdf_filename).stem\n",
    "    \n",
    "    pdf_path = os.path.join(DATA_DIR, pdf_filename)\n",
    "    parsed_md_path = f\"loaddata/gemini_parsed_{base_name}.md\"\n",
    "    chroma_db_dir = f\"./chroma_db/{base_name}\"\n",
    "    parent_store_dir = f\"./parent_store/{base_name}\"\n",
    "    \n",
    "    return {\n",
    "        \"pdf_path\": pdf_path,\n",
    "        \"md_path\": parsed_md_path,\n",
    "        \"chroma_dir\": chroma_db_dir,\n",
    "        \"store_dir\": parent_store_dir,\n",
    "    }\n",
    "\n",
    "# --------------------------------------\n",
    "# Retriever 및 Vectorstore 관리\n",
    "# --------------------------------------\n",
    "retriever_cache = {}\n",
    "\n",
    "def get_retriever_for_pdf(pdf_filename: str):\n",
    "    \"\"\"\n",
    "    선택된 PDF에 대한 retriever를 가져오거나 생성합니다.\n",
    "    - 캐시 확인 -> 없으면 생성 -> 캐시에 저장\n",
    "    - Vectorstore가 비어있으면 문서를 파싱하고 DB를 채웁니다.\n",
    "    \"\"\"\n",
    "    if not pdf_filename:\n",
    "        return None, \"PDF 파일을 선택해주세요.\"\n",
    "\n",
    "    if pdf_filename in retriever_cache:\n",
    "        log_debug(f\"캐시에서 '{pdf_filename}'에 대한 retriever를 로드합니다.\")\n",
    "        return retriever_cache[pdf_filename], f\"'{pdf_filename}'에 대한 준비가 완료되었습니다.\"\n",
    "\n",
    "    paths = get_paths_for_pdf(pdf_filename)\n",
    "    if not paths:\n",
    "        return None, \"경로 생성에 실패했습니다.\"\n",
    "\n",
    "    try:\n",
    "        # 1. Vectorstore 및 Docstore 초기화\n",
    "        vectorstore = Chroma(persist_directory=paths[\"chroma_dir\"], embedding_function=embeddings)\n",
    "        store = JSONDocStore(paths[\"store_dir\"])\n",
    "\n",
    "        # 2. Vectorstore가 비어있는지 확인\n",
    "        if vectorstore._collection.count() == 0:\n",
    "            log_debug(f\"'{paths['chroma_dir']}'가 비어있습니다. 문서 파싱 및 임베딩을 시작합니다.\")\n",
    "            \n",
    "            # 3. (필요 시) PDF 파싱\n",
    "            os.makedirs(os.path.dirname(paths[\"md_path\"]), exist_ok=True)\n",
    "            markdown_file_path = parse_pdf_to_markdown(paths[\"pdf_path\"], output_dir=os.path.dirname(paths[\"md_path\"]))\n",
    "            \n",
    "            with open(markdown_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            documents = [Document(page_content=text, metadata={\"source\": markdown_file_path})]\n",
    "            \n",
    "            # 4. Retriever 생성 및 문서 추가\n",
    "            retriever = ParentDocumentRetriever(\n",
    "                vectorstore=vectorstore,\n",
    "                docstore=store,\n",
    "                child_splitter=child_splitter,\n",
    "                parent_splitter=parent_splitter,\n",
    "                search_kwargs={\"k\": 2},\n",
    "            )\n",
    "            retriever.add_documents(documents)\n",
    "            log_debug(f\"Vector store가 성공적으로 생성되었습니다. Count: {vectorstore._collection.count()}\")\n",
    "\n",
    "        else:\n",
    "            log_debug(f\"기존 vector store를 로드합니다. Count: {vectorstore._collection.count()}\")\n",
    "            retriever = ParentDocumentRetriever(\n",
    "                vectorstore=vectorstore,\n",
    "                docstore=store,\n",
    "                child_splitter=child_splitter,\n",
    "                parent_splitter=parent_splitter,\n",
    "                search_kwargs={\"k\": 2},\n",
    "            )\n",
    "        \n",
    "        # 5. 캐시에 저장\n",
    "        retriever_cache[pdf_filename] = retriever\n",
    "        return retriever, f\"'{pdf_filename}'에 대한 준비가 완료되었습니다.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"'{pdf_filename}' 처리 중 오류 발생: {e}\n",
    "{traceback.format_exc()}\"\"\"\n",
    "        log_debug(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 데이터 로딩 & 벡터DB 적재\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # 내부 속성 의존을 최소화하는 안전한 카운트 함수\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma 내부\n",
    "    except Exception:\n",
    "        try:\n",
    "            # 간단히 비슷문서 조회 시도 (비어있으면 예외 or 빈 결과)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # Chroma는 비어있어도 호출이 성공할 수 있으므로 peek 써봄\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD 파일 생성 (기존 LlamaParse 대신 gemini_parser 사용)\n",
    "    try:\n",
    "        # gemini_parser.py의 함수를 호출합니다.\n",
    "        # 이 함수는 내부적으로 파일 존재 여부를 확인하고, 없으면 PDF를 파싱하여 md 파일을 생성한 후,\n",
    "        # 생성된 마크다운 파일의 최종 경로를 반환합니다.\n",
    "        markdown_file_path = parse_pdf_to_markdown(PDF_PATH, output_dir=os.path.dirname(PARSED_MD_PATH))\n",
    "    except Exception as e:\n",
    "        # GOOGLE_API_KEY가 없거나 다른 오류 발생 시\n",
    "        raise RuntimeError(f\"Gemini Parser를 사용한 PDF 파싱 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "    # md 로드 → Parent retriever에 추가\n",
    "    print(f\"[INFO] Loading markdown from '{markdown_file_path}'...\")\n",
    "    with open(markdown_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 하나의 거대 문서로 추가 → Parent/Child splitter가 내부에서 잘게 쪼갬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 최종 답변(문서 기반만 허용) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"IMPORTANT: You must answer in Korean.\"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 문서 관련성 평가(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 질문 재작성 (웹검색 친화적)\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (선택) 웹검색 도구\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY 미설정: 웹검색 보강은 생략됩니다.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 상태 정의\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]\n",
    "    intent: str  # \"conversational\" or \"question\"\n",
    "    retriever: Optional[ParentDocumentRetriever]\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 새 노드/Chain: 입력 의도 분류\n",
    "# --------------------------------------\n",
    "class ClassifyIntent(BaseModel):\n",
    "    \"\"\"\"conversational\" 또는 \"question\"으로 사용자 입력의 의도를 분류합니다.\"\"\"\n",
    "    intent: str = Field(description=\"사용자 입력의 의도. 'conversational' 또는 'question' 중 하나여야 합니다.\")\n",
    "\n",
    "classify_system_prompt = \"\"\"You are a router that classifies the user's input intent. Based on the user's latest message and the previous conversation history, determine if the input is a simple conversation/chit-chat or a question that requires information.\n",
    "- General greetings like \"Hello\", \"Thank you\", \"Have a nice day\" are 'conversational'.\n",
    "- Responses to previous answers (e.g., \"That's interesting\", \"I see\") are also 'conversational'.\n",
    "- If the input requires finding information from a PDF document or the web, it is a 'question'.\n",
    "- If in doubt, classify it as 'question'.\"\"\"\n",
    "\n",
    "classify_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", classify_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_classifier = llm.with_structured_output(ClassifyIntent)\n",
    "intent_classifier = classify_prompt | structured_llm_classifier\n",
    "\n",
    "\n",
    "def node_classify_input(state: GraphState) -> GraphState:\n",
    "    \"\"\"사용자 입력의 의도를 분류하여 state에 저장\"\"\"\n",
    "    log_debug(\"---CLASSIFYING INPUT INTENT---\")\n",
    "    intent_result = intent_classifier.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    log_debug(f\"  [Intent] Classified as: {intent_result.intent}\")\n",
    "    return {\"intent\": intent_result.intent}\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 새 노드/Chain: 단순 대화형 답변 생성\n",
    "# --------------------------------------\n",
    "conv_gen_system_prompt = \"\"\"You are a friendly AI assistant. Respond to the user's message in a natural, conversational way. Do not search for information; just generate a simple response that fits the context of the conversation. IMPORTANT: You must answer in Korean.\"\"\"\n",
    "\n",
    "conv_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", conv_gen_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "conversational_chain = conv_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def node_generate_conversational_response(state: GraphState) -> GraphState:\n",
    "    \"\"\"단순 대화형 답변을 생성\"\"\"\n",
    "    log_debug(\"---GENERATING CONVERSATIONAL RESPONSE---\")\n",
    "    generation = conversational_chain.invoke({\n",
    "        \"input\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 노드 함수\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"선택된 retriever를 사용하여 문서를 검색합니다.\"\"\"\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    retriever = state.get(\"retriever\")\n",
    "\n",
    "    if not retriever:\n",
    "        raise ValueError(\"Retriever가 설정되지 않았습니다. 문서를 먼저 선택하고 로드해주세요.\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "    # Parent 복구 결과 (History-aware retriever 사용)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(f\"--- Retrieved {len(docs)} documents ---\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # 그레이더 실패 시 일단 보수적으로 유지\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        web_results_text = \"웹검색 API 키가 설정되지 않아 웹검색을 수행하지 못했습니다.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "            # ✅ 결과가 문자열일 때 대비\n",
    "            if isinstance(results, str):\n",
    "                web_results_text = results\n",
    "            elif isinstance(results, list):\n",
    "                lines = []\n",
    "                for r in results:\n",
    "                    if isinstance(r, dict):  # dict 타입만 처리\n",
    "                        title = r.get(\"title\", \"\")\n",
    "                        url = r.get(\"url\", \"\")\n",
    "                        content = r.get(\"content\", \"\")\n",
    "                        lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "                    else:\n",
    "                        lines.append(str(r))  # dict가 아니면 문자열 변환\n",
    "                web_results_text = \"\\n---\\n\".join(lines) if lines else \"검색결과가 비어 있습니다.\"\n",
    "            else:\n",
    "                web_results_text = str(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"웹검색 중 오류: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # 답변 생성\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # 출처 구분\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[출처: 웹검색 결과]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[출처: PDF 문서]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 새 노드: 사용자 알림\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"문서에서 답변을 찾지 못했습니다. 인터넷 검색을 시도합니다.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # ✅ dict로만 유지\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 그래프 구성 & 컴파일\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 1. 새 노드 등록\n",
    "workflow.add_node(\"classify_input\", node_classify_input)\n",
    "workflow.add_node(\"generate_conversational_response\", node_generate_conversational_response)\n",
    "\n",
    "# 2. 기존 노드 등록\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "# 3. 시작점 변경\n",
    "workflow.add_edge(START, \"classify_input\")\n",
    "\n",
    "# 4. 의도에 따른 조건부 분기\n",
    "def decide_flow(state: GraphState) -> str:\n",
    "    log_debug(f\"---DECIDING FLOW BASED ON INTENT: {state['intent']}---\")\n",
    "    if state[\"intent\"] == \"conversational\":\n",
    "        return \"generate_conversational_response\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_flow,\n",
    "    {\n",
    "        \"generate_conversational_response\": \"generate_conversational_response\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 5. 기존 RAG/CRAG 흐름 연결\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"notify_user\": \"notify_user\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "\n",
    "# 6. 종료점 연결\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"generate_conversational_response\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI 및 이벤트 핸들러\n",
    "# --------------------------\n",
    "def get_pdf_list():\n",
    "    \"\"\"'data' 디렉토리에서 PDF 파일 목록을 가져옵니다.\"\"\"\n",
    "    return [f.name for f in pathlib.Path(DATA_DIR).glob(\"*.pdf\")]\n",
    "\n",
    "def handle_file_upload(file):\n",
    "    \"\"\"파일 업로드 시 호출됩니다.\"\"\"\n",
    "    if file is None:\n",
    "        return gr.update(choices=get_pdf_list())\n",
    "    \n",
    "    dest_path = pathlib.Path(DATA_DIR) / pathlib.Path(file.name).name\n",
    "    shutil.copy(file.name, dest_path)\n",
    "    \n",
    "    # 캐시에서 해당 파일이 있다면 삭제하여 리로드를 강제\n",
    "    if dest_path.name in retriever_cache:\n",
    "        del retriever_cache[dest_path.name]\n",
    "        \n",
    "    return gr.update(choices=get_pdf_list(), value=dest_path.name)\n",
    "\n",
    "def handle_pdf_selection(pdf_filename, progress=gr.Progress()):\n",
    "    \"\"\"드롭다운에서 PDF를 선택했을 때 호출됩니다.\"\"\"\n",
    "    if not pdf_filename:\n",
    "        return \"분석할 PDF 파일을 선택해주세요.\", \"\"\n",
    "\n",
    "    progress(0, desc=\"문서 처리 준비 중...\")\n",
    "    retriever, msg = get_retriever_for_pdf(pdf_filename)\n",
    "    progress(1, desc=msg)\n",
    "    \n",
    "    return msg, pdf_filename\n",
    "\n",
    "def run_crag(query: str, history: List[dict], selected_pdf: str, show_debug: bool):\n",
    "    \"\"\"\n",
    "    메인 CRAG 실행 함수. 채팅 메시지 제출 시 호출됩니다.\n",
    "    UI의 모든 입력을 받아 LangGraph를 실행하고 결과를 스트리밍으로 반환합니다.\n",
    "    \"\"\"\n",
    "    global debug_logs\n",
    "    debug_logs = []\n",
    "\n",
    "    # --- 입력 유효성 검사 ---\n",
    "    if not query:\n",
    "        history.append({\"role\": \"user\", \"content\": \"\"})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"질문을 입력해주세요.\"})\n",
    "        yield \"\", history, \"질문을 입력해주세요.\", \"\"\n",
    "        return\n",
    "\n",
    "    if not selected_pdf:\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"먼저 분석할 PDF 문서를 선택해주세요.\"})\n",
    "        yield \"\", history, \"PDF 문서를 선택해주세요.\", \"\"\n",
    "        return\n",
    "\n",
    "    # --- Retriever 준비 ---\n",
    "    retriever, msg = get_retriever_for_pdf(selected_pdf)\n",
    "    if not retriever:\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": f\"문서 준비 중 오류가 발생했습니다: {msg}\"})\n",
    "        yield \"\", history, msg, \"\"\n",
    "        return\n",
    "\n",
    "    # --- LangGraph 실행 ---\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    \n",
    "    try:\n",
    "        inputs = {\n",
    "            \"question\": query, \n",
    "            \"chat_history\": chat_history_for_chain,\n",
    "            \"retriever\": retriever\n",
    "        }\n",
    "        \n",
    "        # 사용자 질문을 히스토리에 먼저 추가\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # 스트리밍 실행 및 답변 생성\n",
    "        generation = \"\"\n",
    "        final_state = {}\n",
    "        for step in app.stream(inputs):\n",
    "            node_name = list(step.keys())[0]\n",
    "            final_state = step[node_name]\n",
    "            log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            \n",
    "            # 웹 검색 시 중간 알림\n",
    "            if node_name == \"grade_documents\" and final_state.get(\"web_search\") == \"Yes\":\n",
    "                history.append({\"role\": \"assistant\", \"content\": \"문서에서 답변을 찾지 못했습니다. 인터넷 검색을 시도합니다.\"})\n",
    "                yield \"\", history, \"웹 검색을 시작합니다...\", \"\"\n",
    "\n",
    "            if \"generation\" in final_state and final_state[\"generation\"]:\n",
    "                 generation = final_state[\"generation\"]\n",
    "\n",
    "        # 최종 답변을 히스토리에 추가/업데이트\n",
    "        if history[-1][\"role\"] == \"assistant\": # 웹 검색 알림이 있었던 경우\n",
    "            history[-1][\"content\"] = generation\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": generation})\n",
    "\n",
    "        # --- 결과 표시 ---\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## 참조 문서\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### 문서 {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"참조된 문서가 없습니다.\"\n",
    "\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        \n",
    "        yield \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"오류 발생: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### 오류\\n```\\n\" + err + \"\\n```\"\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"죄송합니다. 답변 생성 중 오류가 발생했습니다.\"})\n",
    "        yield \"\", history, \"오류가 발생했습니다.\", debug_output\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI 구성\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 Pro는 Gemini 1.5 Pro와 비교했을 때 어떤 점에서 향상되었나요?\",\n",
    "    \"Gemini 2.5 Pro와 Flash는 어떤 종류의 데이터를 처리할 수 있나요?\",\n",
    "    \"Gemini 2.5 시리즈의 작은 모델들은 어떤 방식으로 성능을 개선했나요?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"Dynamic PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# Dynamic PDF RAG + CRAG Chatbot\")\n",
    "    gr.Markdown(\"좌측 상단에서 분석할 PDF를 선택하거나 새 파일을 업로드하세요. 문서가 준비되면 질문을 시작할 수 있습니다.\")\n",
    "\n",
    "    # 현재 선택된 PDF 파일명을 저장하기 위한 상태\n",
    "    selected_pdf_state = gr.State()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # --- 파일 관리 섹션 ---\n",
    "            with gr.Accordion(\"1. 문서 선택 및 관리\", open=True):\n",
    "                pdf_selector = gr.Dropdown(\n",
    "                    label=\"분석할 PDF 문서 선택\",\n",
    "                    choices=get_pdf_list(),\n",
    "                    interactive=True\n",
    "                )\n",
    "                upload_button = gr.UploadButton(\"PDF 업로드\", file_types=[\".pdf\"])\n",
    "                status_display = gr.Markdown(\"대기 중...\")\n",
    "\n",
    "            # --- 채팅 섹션 ---\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요... (Shift+Enter 줄바꿈)\")\n",
    "            \n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"예시 질문\"\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM 참조 문서\")\n",
    "            with gr.Accordion(\"⚙️ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")\n",
    "\n",
    "    # --- 이벤트 핸들러 바인딩 ---\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel, status_display])\n",
    "\n",
    "    # 1. 파일 업로드 시: 파일을 서버에 저장하고, 드롭다운 목록을 갱신\n",
    "    upload_button.upload(handle_file_upload, inputs=[upload_button], outputs=[pdf_selector])\n",
    "\n",
    "    # 2. 드롭다운에서 PDF 선택 시: 해당 PDF에 대한 retriever를 준비/로드\n",
    "    pdf_selector.change(\n",
    "        handle_pdf_selection, \n",
    "        inputs=[pdf_selector], \n",
    "        outputs=[status_display, selected_pdf_state]\n",
    "    )\n",
    "\n",
    "    # 3. 메시지 전송 시: CRAG 파이프라인 실행\n",
    "    msg.submit(\n",
    "        run_crag, \n",
    "        [msg, chatbot, selected_pdf_state, show_debug_checkbox],\n",
    "        [msg, chatbot, context_display, debug_panel]\n",
    "    )\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bd54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
