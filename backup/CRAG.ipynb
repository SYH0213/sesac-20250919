{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e598d47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08f1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# --- Document Loading and Caching ---\n",
    "PDF_PATH = \"data/gemini-2.5-tech_3.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_3.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ea157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Splitters\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# 2. Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3. Vectorstore\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# 4. ParentDocumentRetriever\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\":2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852c1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_populate_vectorstore():\n",
    "    if vectorstore._collection.count() > 0:\n",
    "        return\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        parser = LlamaParse(result_type=\"markdown\", api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"))\n",
    "        documents = parser.load_data(PDF_PATH)\n",
    "        with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join([doc.text for doc in documents]))\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    retriever.add_documents([Document(page_content=text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851f6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prompt for History-Aware Retriever\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b6900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the History-Aware Retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de30e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prompt for Final Answer Generation\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your ONLY task is to answer the user's question STRICTLY based on the provided context. \"\n",
    "    \"If the information to answer the question is present in the context, provide a concise answer. \"\n",
    "    \"If the answer cannot be found within the provided context, you MUST say 'ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' Do NOT use any of your outside knowledge.\"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the Document Chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --- RAG Function ---\n",
    "def ask_llm(query, history):\n",
    "    chat_history_for_chain = []\n",
    "    if history:\n",
    "        for message in history:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                chat_history_for_chain.append(HumanMessage(content=message[\"content\"]))\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                chat_history_for_chain.append(AIMessage(content=message[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d505f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00425235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1914: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23071e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fc09a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is agent memory and how does it function in artificial intelligence?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question = \"agent memory\"\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e570497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "# --- GraphState ì •ì˜ ---\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    chat_history: list\n",
    "\n",
    "# --- Node í•¨ìˆ˜ë“¤ ---\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # history-aware retriever ì‚¬ìš©\n",
    "    retrieved_docs = history_aware_retriever.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return {\"documents\": retrieved_docs, \"question\": question, \"chat_history\": chat_history}\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        if score.binary_score == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": answer, \"chat_history\": chat_history}\n",
    "\n",
    "def transform_query(state):\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": state[\"documents\"], \"question\": better_question, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def web_search_node(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    documents = state[\"documents\"]\n",
    "    documents.append(Document(page_content=web_results))\n",
    "    return {\"documents\": documents, \"question\": question, \"chat_history\": state[\"chat_history\"]}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    if state[\"web_search\"] == \"Yes\":\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "\n",
    "# --- ê·¸ë˜í”„ êµ¬ì„± ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search_node)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\"transform_query\": \"transform_query\", \"generate\": \"generate\"}\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6ec19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "{'retrieve': {'documents': [], 'question': 'Gemini 2.5 ProëŠ” ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜ìš”?', 'chat_history': []}}\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "{'grade_documents': {'documents': [], 'question': 'Gemini 2.5 ProëŠ” ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜ìš”?', 'web_search': 'No', 'chat_history': []}}\n",
      "---GENERATE---\n",
      "{'generate': {'documents': [], 'question': 'Gemini 2.5 ProëŠ” ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜ìš”?', 'generation': 'ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'chat_history': []}}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Gemini 2.5 ProëŠ” ë¬´ì—‡ì´ ë‹¬ë¼ì¡Œë‚˜ìš”?\", \"chat_history\": []}\n",
    "for output in app.stream(inputs):\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a7d9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "382bada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) í†µí•© ë²„ì „ (Gradio UI)\n",
    "# - PDF â†’ LlamaParse(md) â†’ Chroma(Parent/Child) â†’ History-Aware Retrieve\n",
    "# - CRAG: grade_documents â†’ (generate | transform_query â†’ web_search â†’ generate)\n",
    "# - ë¬¸ì„œ ì™¸ ì§€ì‹ ê¸ˆì§€, ì—†ìœ¼ë©´ í•œêµ­ì–´ë¡œ \"ì œê³µëœ ë¬¸ì„œ...\" ì¶œë ¥\n",
    "# - ì›¹ê²€ìƒ‰: Tavily(ì„ íƒ, ë¯¸ì„¤ì • ì‹œ ìš°íšŒ)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # âœ… BaseStoreëŠ” ì—¬ê¸°ë¡œ ì´ë™ë¨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) ì›¹ ê²€ìƒ‰ íˆ´\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# ìœ í‹¸: Gradioìš© íˆìŠ¤í† ë¦¬ ë³€í™˜\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì „ì—­ Debug Log ì €ì¥ì†Œ\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # ì½˜ì†”ì—ë„ ê·¸ëŒ€ë¡œ ì°ì–´ì¤Œ\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore.\n",
    "    - key -> ./parent_store/{key}.json ì— Document ì €ì¥\n",
    "    - ParentDocumentRetriever ê°€ ìš”êµ¬í•˜ëŠ” mset/mget/mdelete/yield_keys êµ¬í˜„\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ & ì„¤ì •\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "# --------------------------\n",
    "# ê²½ë¡œ ë° ì „ì—­ ì„¤ì •\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & ì„ë² ë”©\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# ê¸°ì¡´: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # íŒŒì¼ ê¸°ë°˜ parent ì €ì¥\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ë°ì´í„° ë¡œë”© & ë²¡í„°DB ì ì¬\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # ë‚´ë¶€ ì†ì„± ì˜ì¡´ì„ ìµœì†Œí™”í•˜ëŠ” ì•ˆì „í•œ ì¹´ìš´íŠ¸ í•¨ìˆ˜\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma ë‚´ë¶€\n",
    "    except Exception:\n",
    "        try:\n",
    "            # ê°„ë‹¨íˆ ë¹„ìŠ·ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (ë¹„ì–´ìˆìœ¼ë©´ ì˜ˆì™¸ or ë¹ˆ ê²°ê³¼)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # ChromaëŠ” ë¹„ì–´ìˆì–´ë„ í˜¸ì¶œì´ ì„±ê³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ peek ì¨ë´„\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD íŒŒì¼ ì—†ìœ¼ë©´ PDF â†’ LlamaParse â†’ md ì €ì¥\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEYê°€ ì—†ì–´ PDF íŒŒì‹±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    # md ë¡œë“œ â†’ Parent retrieverì— ì¶”ê°€\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # í•˜ë‚˜ì˜ ê±°ëŒ€ ë¬¸ì„œë¡œ ì¶”ê°€ â†’ Parent/Child splitterê°€ ë‚´ë¶€ì—ì„œ ì˜ê²Œ ìª¼ê°¬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# ìµœì¢… ë‹µë³€(ë¬¸ì„œ ê¸°ë°˜ë§Œ í—ˆìš©) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ì§ˆë¬¸ ì¬ì‘ì„± (ì›¹ê²€ìƒ‰ ì¹œí™”ì )\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (ì„ íƒ) ì›¹ê²€ìƒ‰ ë„êµ¬\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY ë¯¸ì„¤ì •: ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ìƒëµë©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]  # í•­ìƒ HumanMessage/AIMessageë§Œ\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ë…¸ë“œ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # ì› ì§ˆë¬¸ + íˆìŠ¤í† ë¦¬ ì¶œë ¥\n",
    "    log_debug(f\"[DEBUG] Raw Question: {question}\")\n",
    "    if chat_history:\n",
    "        log_debug(f\"[DEBUG] Chat History Count: {len(chat_history)}\")\n",
    "    else:\n",
    "        log_debug(\"[DEBUG] No chat history provided.\")\n",
    "\n",
    "    # Child ê²€ìƒ‰ ê²°ê³¼ í™•ì¸\n",
    "    child_results = vectorstore.similarity_search(question, k=2)\n",
    "    log_debug(\"=== Child ê²€ìƒ‰ ê²°ê³¼ ===\")\n",
    "    for i, d in enumerate(child_results, 1):\n",
    "        log_debug(f\"[Child {i}] Parent ID: {d.metadata.get('doc_id')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:200]}...\\n\")\n",
    "\n",
    "    # Parent ë³µêµ¬ ê²°ê³¼ (History-aware retriever ì‚¬ìš©)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(\"=== Parent ë³µêµ¬ ê²°ê³¼ ===\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        log_debug(f\"[Parent {i}] Source: {d.metadata.get('source', 'N/A')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:500]}...\\n\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # ê·¸ë ˆì´ë” ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        # ì›¹ê²€ìƒ‰ ë¶ˆê°€ ì‹œ ì•ˆë‚´ ë¬¸ì„œ ì¶”ê°€\n",
    "        web_results_text = \"ì›¹ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "            lines = []\n",
    "            for r in results:\n",
    "                # r keys: content, url, score, title ë“±\n",
    "                title = r.get(\"title\") or \"\"\n",
    "                url = r.get(\"url\") or \"\"\n",
    "                content = r.get(\"content\") or \"\"\n",
    "                lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "            web_results_text = \"\\n---\\n\".join(lines) if lines else \"ê²€ìƒ‰ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"ì›¹ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # ì¶œì²˜ êµ¬ë¶„\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: ì›¹ê²€ìƒ‰ ê²°ê³¼]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: PDF ë¬¸ì„œ]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ìƒˆ ë…¸ë“œ: ì‚¬ìš©ì ì•Œë¦¼\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # âœ… dictë¡œë§Œ ìœ ì§€\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ê·¸ë˜í”„ êµ¬ì„± & ì»´íŒŒì¼\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)   # âœ… ì¶”ê°€\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# ì¡°ê±´ ë¶„ê¸° ìˆ˜ì •\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",            # ë¬¸ì„œ ê´€ë ¨ì„± O â†’ ë°”ë¡œ ë‹µë³€\n",
    "        \"notify_user\": \"notify_user\",  # ë¬¸ì„œ ê´€ë ¨ì„± X â†’ ë¨¼ì € notify_user\n",
    "    },\n",
    ")\n",
    "\n",
    "# ì•Œë¦¼ í›„ ì›¹ê²€ìƒ‰ ë¶„ê¸° ì—°ê²°\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# run_crag ìˆ˜ì •\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict], show_debug: bool):\n",
    "    global debug_logs\n",
    "    debug_logs = []  # ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°í™”\n",
    "\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain,\n",
    "                  \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            for node_name, node_state in step.items():\n",
    "                log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state\n",
    "\n",
    "        # ìµœì¢… ì‘ë‹µ\n",
    "        answer = final_state.get(\"generation\", \"ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## ì°¸ì¡° ë¬¸ì„œ\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### ë¬¸ì„œ {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        # íˆìŠ¤í† ë¦¬ ì¶”ê°€\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        # ë””ë²„ê·¸ í‘œì‹œ ì—¬ë¶€ ê²°ì •\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        return \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"ì˜¤ë¥˜ ë°œìƒ: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### ì˜¤ë¥˜\\n```\\n\" + err + \"\\n```\"\n",
    "        return \"\", history, \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\", debug_output\n",
    "\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # ì „ì²´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"âœ… Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì´ˆê¸° ì ì¬\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 ì‹œë¦¬ì¦ˆì˜ ì‘ì€ ëª¨ë¸ë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‚˜ìš”?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. ë¬¸ì„œì—ì„œ ëª» ì°¾ìœ¼ë©´ ì§ˆë¬¸ ì¬ì‘ì„± + (ì„ íƒ)ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ------------------------------\n",
    "        # ì™¼ìª½: ì±„íŒ… ì˜ì—­\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"ì˜ˆì‹œ ì§ˆë¬¸\"\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        # ì˜¤ë¥¸ìª½: ë¬¸ì„œ/ì˜µì…˜/ë””ë²„ê·¸ ì˜ì—­\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM ì°¸ì¡° ë¬¸ì„œ ì „ë¬¸/ìš”ì•½\")\n",
    "\n",
    "            with gr.Accordion(\"âš™ï¸ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")   # âœ… ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ íŒ¨ë„\n",
    "                reload_button = gr.Button(\"ğŸ”„ Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    # ------------------------------\n",
    "    # ë²„íŠ¼/ì´ë²¤íŠ¸ ë°”ì¸ë”©\n",
    "    # ------------------------------\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel])\n",
    "    msg.submit(run_crag, [msg, chatbot, show_debug_checkbox],\n",
    "               [msg, chatbot, context_display, debug_panel])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f99de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vector store already populated. Count=174\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) í†µí•© ë²„ì „ (Gradio UI)\n",
    "# - PDF â†’ LlamaParse(md) â†’ Chroma(Parent/Child) â†’ History-Aware Retrieve\n",
    "# - CRAG: grade_documents â†’ (generate | transform_query â†’ web_search â†’ generate)\n",
    "# - ë¬¸ì„œ ì™¸ ì§€ì‹ ê¸ˆì§€, ì—†ìœ¼ë©´ í•œêµ­ì–´ë¡œ \"ì œê³µëœ ë¬¸ì„œ...\" ì¶œë ¥\n",
    "# - ì›¹ê²€ìƒ‰: Tavily(ì„ íƒ, ë¯¸ì„¤ì • ì‹œ ìš°íšŒ)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # âœ… BaseStoreëŠ” ì—¬ê¸°ë¡œ ì´ë™ë¨\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) ì›¹ ê²€ìƒ‰ íˆ´\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore.\n",
    "    - key -> ./parent_store/{key}.json ì— Document ì €ì¥\n",
    "    - ParentDocumentRetriever ê°€ ìš”êµ¬í•˜ëŠ” mset/mget/mdelete/yield_keys êµ¬í˜„\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ & ì„¤ì •\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "# --------------------------\n",
    "# ê²½ë¡œ ë° ì „ì—­ ì„¤ì •\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & ì„ë² ë”©\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# ê¸°ì¡´: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # íŒŒì¼ ê¸°ë°˜ parent ì €ì¥\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ë°ì´í„° ë¡œë”© & ë²¡í„°DB ì ì¬\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # ë‚´ë¶€ ì†ì„± ì˜ì¡´ì„ ìµœì†Œí™”í•˜ëŠ” ì•ˆì „í•œ ì¹´ìš´íŠ¸ í•¨ìˆ˜\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma ë‚´ë¶€\n",
    "    except Exception:\n",
    "        try:\n",
    "            # ê°„ë‹¨íˆ ë¹„ìŠ·ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (ë¹„ì–´ìˆìœ¼ë©´ ì˜ˆì™¸ or ë¹ˆ ê²°ê³¼)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # ChromaëŠ” ë¹„ì–´ìˆì–´ë„ í˜¸ì¶œì´ ì„±ê³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ peek ì¨ë´„\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD íŒŒì¼ ì—†ìœ¼ë©´ PDF â†’ LlamaParse â†’ md ì €ì¥\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEYê°€ ì—†ì–´ PDF íŒŒì‹±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    # md ë¡œë“œ â†’ Parent retrieverì— ì¶”ê°€\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # í•˜ë‚˜ì˜ ê±°ëŒ€ ë¬¸ì„œë¡œ ì¶”ê°€ â†’ Parent/Child splitterê°€ ë‚´ë¶€ì—ì„œ ì˜ê²Œ ìª¼ê°¬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# ìµœì¢… ë‹µë³€(ë¬¸ì„œ ê¸°ë°˜ë§Œ í—ˆìš©) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"If the answer cannot be found within the provided context, you MUST say 'ì œê³µëœ ë¬¸ì„œë‚˜ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ì§ˆë¬¸ ì¬ì‘ì„± (ì›¹ê²€ìƒ‰ ì¹œí™”ì )\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (ì„ íƒ) ì›¹ê²€ìƒ‰ ë„êµ¬\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY ë¯¸ì„¤ì •: ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ìƒëµë©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List  # HumanMessage/AIMessage ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ë…¸ë“œ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    docs = history_aware_retriever.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "    return {\"documents\": docs, \"question\": question, \"chat_history\": chat_history, \"web_search\": \"No\", \"generation\": \"\"}\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # ê·¸ë ˆì´ë” ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"transform_query\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        # ì›¹ê²€ìƒ‰ ë¶ˆê°€ ì‹œ ì•ˆë‚´ ë¬¸ì„œ ì¶”ê°€\n",
    "        web_results_text = \"ì›¹ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "            lines = []\n",
    "            for r in results:\n",
    "                # r keys: content, url, score, title ë“±\n",
    "                title = r.get(\"title\") or \"\"\n",
    "                url = r.get(\"url\") or \"\"\n",
    "                content = r.get(\"content\") or \"\"\n",
    "                lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "            web_results_text = \"\\n---\\n\".join(lines) if lines else \"ê²€ìƒ‰ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"ì›¹ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # ì¶œì²˜ êµ¬ë¶„\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: ì›¹ê²€ìƒ‰ ê²°ê³¼]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: PDF ë¬¸ì„œ]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ê·¸ë˜í”„ êµ¬ì„± & ì»´íŒŒì¼\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\"transform_query\": \"transform_query\", \"generate\": \"generate\"},\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ìœ í‹¸: Gradioìš© íˆìŠ¤í† ë¦¬ ë³€í™˜\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio í•¸ë“¤ëŸ¬\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict]):\n",
    "    # 1) Chat history ë³€í™˜\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "\n",
    "    # 2) ê·¸ë˜í”„ ì‹¤í–‰\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain, \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            # ë””ë²„ê¹… ë¡œê·¸(ì„ íƒ)\n",
    "            for node_name, node_state in step.items():\n",
    "                print(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state  # ë§ˆì§€ë§‰ state\n",
    "\n",
    "        # 3) ìµœì¢… ì‘ë‹µ/ë¬¸ì„œ ì •ë¦¬\n",
    "        answer = final_state.get(\"generation\", \"ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## ì°¸ì¡° ë¬¸ì„œ\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:800] + (\"...\" if len(d.page_content) > 800 else \"\")\n",
    "                context_md += f\"### ë¬¸ì„œ {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        # 4) íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        return \"\", history, context_md\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"ì˜¤ë¥˜ ë°œìƒ: {e}\\n{traceback.format_exc()}\"\n",
    "        if history is None:\n",
    "            history = []\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì½˜ì†” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\"})\n",
    "        return \"\", history, f\"### ì˜¤ë¥˜\\n```\\n{err}\\n```\"\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # ì „ì²´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"âœ… Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì´ˆê¸° ì ì¬\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 ì‹œë¦¬ì¦ˆì˜ ì‘ì€ ëª¨ë¸ë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‚˜ìš”?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. ë¬¸ì„œì—ì„œ ëª» ì°¾ìœ¼ë©´ ì§ˆë¬¸ ì¬ì‘ì„± + (ì„ íƒ)ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)\")\n",
    "\n",
    "            gr.Examples(examples=example_questions, inputs=msg, label=\"ì˜ˆì‹œ ì§ˆë¬¸\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            context_display = gr.Markdown(label=\"LLM ì°¸ì¡° ë¬¸ì„œ ì „ë¬¸/ìš”ì•½\")\n",
    "            with gr.Accordion(\"âš™ï¸ Advanced Options\", open=False):\n",
    "                reload_button = gr.Button(\"ğŸ”„ Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display])\n",
    "    msg.submit(run_crag, [msg, chatbot], [msg, chatbot, context_display])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8d3ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================================\n",
    "# íŒŒì¼ ì„¤ëª…: CRAGv4.py\n",
    "#\n",
    "# ê¸°ëŠ¥:\n",
    "# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PDF ë¬¸ì„œ ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡ì„ Gradio ì›¹ UIë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "# LangChainê³¼ LangGraphë¥¼ ê¸°ë°˜ìœ¼ë¡œ Corrective RAG (CRAG) ë° ì¼ë°˜ RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•˜ì—¬,\n",
    "# ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì‘ë‹µ ë°©ì‹ì„ ë³€ê²½í•˜ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "#\n",
    "# ì£¼ìš” ì•„í‚¤í…ì²˜ ë° ê¸°ëŠ¥:\n",
    "# 1.  **ë¬¸ì„œ ì²˜ë¦¬**:\n",
    "#     - LlamaParseë¥¼ ì‚¬ìš©í•´ PDF ë¬¸ì„œë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "#     - ì²˜ë¦¬ëœ Markdownì€ ì´í›„ ì‹¤í–‰ ì‹œ ì¬ì‚¬ìš©ì„ ìœ„í•´ ìºì‹±ë©ë‹ˆë‹¤.\n",
    "#\n",
    "# 2.  **RAG (Retrieval-Augmented Generation)**:\n",
    "#     - `ParentDocumentRetriever`: ë¬¸ì„œë¥¼ ë¶€ëª¨/ìì‹ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.\n",
    "#     - `ChromaDB`: ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©ë˜ë©°, ì„ë² ë”©ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "#     - `History-Aware Retriever`: ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ì—¬ í›„ì† ì§ˆë¬¸(ì˜ˆ: \"ê·¸ê±´ ì–´ë•Œ?\")ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "#\n",
    "# 3.  **CRAG (Corrective RAG) & ë¼ìš°íŒ… (LangGraph ê¸°ë°˜)**:\n",
    "#     - **ì˜ë„ ë¶„ë¥˜ (Intent Classification)**: ì‚¬ìš©ìì˜ ì…ë ¥ì„ 'ë‹¨ìˆœ ëŒ€í™”'ì™€ 'ì •ë³´ì„± ì§ˆë¬¸'ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°íŒ… ë…¸ë“œë¥¼ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "#     - **ëŒ€í™”í˜• ì‘ë‹µ**: 'ë‹¨ìˆœ ëŒ€í™”'ë¡œ ë¶„ë¥˜ë˜ë©´, RAG íŒŒì´í”„ë¼ì¸ì„ ê±´ë„ˆë›°ê³  LLMì´ ì§ì ‘ ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "#     - **ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€**: 'ì •ë³´ì„± ì§ˆë¬¸'ì˜ ê²½ìš°, ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ LLMì´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "#     - **ì›¹ ê²€ìƒ‰ ë³´ê°•**: ê´€ë ¨ ë¬¸ì„œê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´, ì§ˆë¬¸ì„ ì›¹ ê²€ìƒ‰ì— ë” ì í•©í•˜ê²Œ ë³€í˜•í•œ í›„ Tavily APIë¥¼ í†µí•´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "#\n",
    "# 4.  **ë©”ëª¨ë¦¬ ê´€ë¦¬**:\n",
    "#     - Stateless êµ¬ì¡°: ì„œë²„ëŠ” ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë©°, ë§¤ ìš”ì²­ë§ˆë‹¤ Gradio UI(ë¸Œë¼ìš°ì €)ë¡œë¶€í„° ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ì „ë‹¬ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "#     - UI ì•Œë¦¼ ë²„ê·¸ ìˆ˜ì •: ì›¹ ê²€ìƒ‰ ì‹œ 'ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤'ì™€ ê°™ì€ ì¤‘ê°„ ê³¼ì •ì˜ ì•Œë¦¼ì´ UIì— ì •ìƒì ìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ `run_crag` í•¨ìˆ˜ì˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë¡œì§ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.\n",
    "#\n",
    "# 5.  **UI**:\n",
    "#     - Gradioë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì‰½ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "# =================================================================================================\n",
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) í†µí•© ë²„ì „ (Gradio UI)\n",
    "# - PDF â†’ LlamaParse(md) â†’ Chroma(Parent/Child) â†’ History-Aware Retrieve\n",
    "# - CRAG: grade_documents â†’ (generate | transform_query â†’ web_search â†’ generate)\n",
    "# - ë¬¸ì„œ ì™¸ ì§€ì‹ ê¸ˆì§€, ì—†ìœ¼ë©´ í•œêµ­ì–´ë¡œ \"ì œê³µëœ ë¬¸ì„œ...\" ì¶œë ¥\n",
    "# - ì›¹ê²€ìƒ‰: Tavily(ì„ íƒ, ë¯¸ì„¤ì • ì‹œ ìš°íšŒ)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from gemini_parser import parse_pdf_to_markdown\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # âœ… BaseStoreëŠ” ì—¬ê¸°ë¡œ ì´ë™ë¨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) ì›¹ ê²€ìƒ‰ íˆ´\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# ìœ í‹¸: Gradioìš© íˆìŠ¤í† ë¦¬ ë³€í™˜\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:\n",
    "    history = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì „ì—­ Debug Log ì €ì¥ì†Œ\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # ì½˜ì†”ì—ë„ ê·¸ëŒ€ë¡œ ì°ì–´ì¤Œ\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore.\n",
    "    - key -> ./parent_store/{key}.json ì— Document ì €ì¥\n",
    "    - ParentDocumentRetriever ê°€ ìš”êµ¬í•˜ëŠ” mset/mget/mdelete/yield_keys êµ¬í˜„\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ & ì„¤ì •\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "# --------------------------\n",
    "# ê²½ë¡œ ë° ì „ì—­ ì„¤ì •\n",
    "# --------------------------\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# LLM & ì„ë² ë”©\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------------------\n",
    "# ë™ì  ê²½ë¡œ ê´€ë¦¬ í—¬í¼\n",
    "# --------------------------------------\n",
    "def get_paths_for_pdf(pdf_filename: str):\n",
    "    \"\"\"ì„ íƒëœ PDF íŒŒì¼ëª…ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²½ë¡œë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not pdf_filename:\n",
    "        return None\n",
    "    \n",
    "    base_name = pathlib.Path(pdf_filename).stem\n",
    "    \n",
    "    pdf_path = os.path.join(DATA_DIR, pdf_filename)\n",
    "    parsed_md_path = f\"loaddata/gemini_parsed_{base_name}.md\"\n",
    "    chroma_db_dir = f\"./chroma_db/{base_name}\"\n",
    "    parent_store_dir = f\"./parent_store/{base_name}\"\n",
    "    \n",
    "    return {\n",
    "        \"pdf_path\": pdf_path,\n",
    "        \"md_path\": parsed_md_path,\n",
    "        \"chroma_dir\": chroma_db_dir,\n",
    "        \"store_dir\": parent_store_dir,\n",
    "    }\n",
    "\n",
    "# --------------------------------------\n",
    "# Retriever ë° Vectorstore ê´€ë¦¬\n",
    "# --------------------------------------\n",
    "retriever_cache = {}\n",
    "\n",
    "def get_retriever_for_pdf(pdf_filename: str):\n",
    "    \"\"\"\n",
    "    ì„ íƒëœ PDFì— ëŒ€í•œ retrieverë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    - ìºì‹œ í™•ì¸ -> ì—†ìœ¼ë©´ ìƒì„± -> ìºì‹œì— ì €ì¥\n",
    "    - Vectorstoreê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê³  DBë¥¼ ì±„ì›ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not pdf_filename:\n",
    "        return None, \"PDF íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "    if pdf_filename in retriever_cache:\n",
    "        log_debug(f\"ìºì‹œì—ì„œ '{pdf_filename}'ì— ëŒ€í•œ retrieverë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "        return retriever_cache[pdf_filename], f\"'{pdf_filename}'ì— ëŒ€í•œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    paths = get_paths_for_pdf(pdf_filename)\n",
    "    if not paths:\n",
    "        return None, \"ê²½ë¡œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    try:\n",
    "        # 1. Vectorstore ë° Docstore ì´ˆê¸°í™”\n",
    "        vectorstore = Chroma(persist_directory=paths[\"chroma_dir\"], embedding_function=embeddings)\n",
    "        store = JSONDocStore(paths[\"store_dir\"])\n",
    "\n",
    "        # 2. Vectorstoreê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸\n",
    "        if vectorstore._collection.count() == 0:\n",
    "            log_debug(f\"'{paths['chroma_dir']}'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œ íŒŒì‹± ë° ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "            \n",
    "            # 3. (í•„ìš” ì‹œ) PDF íŒŒì‹±\n",
    "            os.makedirs(os.path.dirname(paths[\"md_path\"]), exist_ok=True)\n",
    "            markdown_file_path = parse_pdf_to_markdown(paths[\"pdf_path\"], output_dir=os.path.dirname(paths[\"md_path\"]))\n",
    "            \n",
    "            with open(markdown_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            documents = [Document(page_content=text, metadata={\"source\": markdown_file_path})]\n",
    "            \n",
    "            # 4. Retriever ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€\n",
    "            retriever = ParentDocumentRetriever(\n",
    "                vectorstore=vectorstore,\n",
    "                docstore=store,\n",
    "                child_splitter=child_splitter,\n",
    "                parent_splitter=parent_splitter,\n",
    "                search_kwargs={\"k\": 2},\n",
    "            )\n",
    "            retriever.add_documents(documents)\n",
    "            log_debug(f\"Vector storeê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. Count: {vectorstore._collection.count()}\")\n",
    "\n",
    "        else:\n",
    "            log_debug(f\"ê¸°ì¡´ vector storeë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. Count: {vectorstore._collection.count()}\")\n",
    "            retriever = ParentDocumentRetriever(\n",
    "                vectorstore=vectorstore,\n",
    "                docstore=store,\n",
    "                child_splitter=child_splitter,\n",
    "                parent_splitter=parent_splitter,\n",
    "                search_kwargs={\"k\": 2},\n",
    "            )\n",
    "        \n",
    "        # 5. ìºì‹œì— ì €ì¥\n",
    "        retriever_cache[pdf_filename] = retriever\n",
    "        return retriever, f\"'{pdf_filename}'ì— ëŒ€í•œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"'{pdf_filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n",
    "{traceback.format_exc()}\"\"\"\n",
    "        log_debug(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ë°ì´í„° ë¡œë”© & ë²¡í„°DB ì ì¬\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # ë‚´ë¶€ ì†ì„± ì˜ì¡´ì„ ìµœì†Œí™”í•˜ëŠ” ì•ˆì „í•œ ì¹´ìš´íŠ¸ í•¨ìˆ˜\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma ë‚´ë¶€\n",
    "    except Exception:\n",
    "        try:\n",
    "            # ê°„ë‹¨íˆ ë¹„ìŠ·ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (ë¹„ì–´ìˆìœ¼ë©´ ì˜ˆì™¸ or ë¹ˆ ê²°ê³¼)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # ChromaëŠ” ë¹„ì–´ìˆì–´ë„ í˜¸ì¶œì´ ì„±ê³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ peek ì¨ë´„\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD íŒŒì¼ ìƒì„± (ê¸°ì¡´ LlamaParse ëŒ€ì‹  gemini_parser ì‚¬ìš©)\n",
    "    try:\n",
    "        # gemini_parser.pyì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "        # ì´ í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ PDFë¥¼ íŒŒì‹±í•˜ì—¬ md íŒŒì¼ì„ ìƒì„±í•œ í›„,\n",
    "        # ìƒì„±ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ìµœì¢… ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        markdown_file_path = parse_pdf_to_markdown(PDF_PATH, output_dir=os.path.dirname(PARSED_MD_PATH))\n",
    "    except Exception as e:\n",
    "        # GOOGLE_API_KEYê°€ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "        raise RuntimeError(f\"Gemini Parserë¥¼ ì‚¬ìš©í•œ PDF íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "    # md ë¡œë“œ â†’ Parent retrieverì— ì¶”ê°€\n",
    "    print(f\"[INFO] Loading markdown from '{markdown_file_path}'...\")\n",
    "    with open(markdown_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # í•˜ë‚˜ì˜ ê±°ëŒ€ ë¬¸ì„œë¡œ ì¶”ê°€ â†’ Parent/Child splitterê°€ ë‚´ë¶€ì—ì„œ ì˜ê²Œ ìª¼ê°¬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# ìµœì¢… ë‹µë³€(ë¬¸ì„œ ê¸°ë°˜ë§Œ í—ˆìš©) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"IMPORTANT: You must answer in Korean.\"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ì§ˆë¬¸ ì¬ì‘ì„± (ì›¹ê²€ìƒ‰ ì¹œí™”ì )\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (ì„ íƒ) ì›¹ê²€ìƒ‰ ë„êµ¬\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY ë¯¸ì„¤ì •: ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ìƒëµë©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]\n",
    "    intent: str  # \"conversational\" or \"question\"\n",
    "    retriever: Optional[ParentDocumentRetriever]\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ìƒˆ ë…¸ë“œ/Chain: ì…ë ¥ ì˜ë„ ë¶„ë¥˜\n",
    "# --------------------------------------\n",
    "class ClassifyIntent(BaseModel):\n",
    "    \"\"\"\"conversational\" ë˜ëŠ” \"question\"ìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    intent: str = Field(description=\"ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„. 'conversational' ë˜ëŠ” 'question' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "classify_system_prompt = \"\"\"You are a router that classifies the user's input intent. Based on the user's latest message and the previous conversation history, determine if the input is a simple conversation/chit-chat or a question that requires information.\n",
    "- General greetings like \"Hello\", \"Thank you\", \"Have a nice day\" are 'conversational'.\n",
    "- Responses to previous answers (e.g., \"That's interesting\", \"I see\") are also 'conversational'.\n",
    "- If the input requires finding information from a PDF document or the web, it is a 'question'.\n",
    "- If in doubt, classify it as 'question'.\"\"\"\n",
    "\n",
    "classify_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", classify_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_classifier = llm.with_structured_output(ClassifyIntent)\n",
    "intent_classifier = classify_prompt | structured_llm_classifier\n",
    "\n",
    "\n",
    "def node_classify_input(state: GraphState) -> GraphState:\n",
    "    \"\"\"ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì—¬ stateì— ì €ì¥\"\"\"\n",
    "    log_debug(\"---CLASSIFYING INPUT INTENT---\")\n",
    "    intent_result = intent_classifier.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    log_debug(f\"  [Intent] Classified as: {intent_result.intent}\")\n",
    "    return {\"intent\": intent_result.intent}\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ìƒˆ ë…¸ë“œ/Chain: ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ ìƒì„±\n",
    "# --------------------------------------\n",
    "conv_gen_system_prompt = \"\"\"You are a friendly AI assistant. Respond to the user's message in a natural, conversational way. Do not search for information; just generate a simple response that fits the context of the conversation. IMPORTANT: You must answer in Korean.\"\"\"\n",
    "\n",
    "conv_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", conv_gen_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "conversational_chain = conv_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def node_generate_conversational_response(state: GraphState) -> GraphState:\n",
    "    \"\"\"ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±\"\"\"\n",
    "    log_debug(\"---GENERATING CONVERSATIONAL RESPONSE---\")\n",
    "    generation = conversational_chain.invoke({\n",
    "        \"input\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ë…¸ë“œ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"ì„ íƒëœ retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    retriever = state.get(\"retriever\")\n",
    "\n",
    "    if not retriever:\n",
    "        raise ValueError(\"Retrieverê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì„ íƒí•˜ê³  ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "    # Parent ë³µêµ¬ ê²°ê³¼ (History-aware retriever ì‚¬ìš©)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(f\"--- Retrieved {len(docs)} documents ---\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # ê·¸ë ˆì´ë” ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        web_results_text = \"ì›¹ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "            # âœ… ê²°ê³¼ê°€ ë¬¸ìì—´ì¼ ë•Œ ëŒ€ë¹„\n",
    "            if isinstance(results, str):\n",
    "                web_results_text = results\n",
    "            elif isinstance(results, list):\n",
    "                lines = []\n",
    "                for r in results:\n",
    "                    if isinstance(r, dict):  # dict íƒ€ì…ë§Œ ì²˜ë¦¬\n",
    "                        title = r.get(\"title\", \"\")\n",
    "                        url = r.get(\"url\", \"\")\n",
    "                        content = r.get(\"content\", \"\")\n",
    "                        lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "                    else:\n",
    "                        lines.append(str(r))  # dictê°€ ì•„ë‹ˆë©´ ë¬¸ìì—´ ë³€í™˜\n",
    "                web_results_text = \"\\n---\\n\".join(lines) if lines else \"ê²€ìƒ‰ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "            else:\n",
    "                web_results_text = str(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"ì›¹ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # ì¶œì²˜ êµ¬ë¶„\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: ì›¹ê²€ìƒ‰ ê²°ê³¼]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: PDF ë¬¸ì„œ]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ìƒˆ ë…¸ë“œ: ì‚¬ìš©ì ì•Œë¦¼\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # âœ… dictë¡œë§Œ ìœ ì§€\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ê·¸ë˜í”„ êµ¬ì„± & ì»´íŒŒì¼\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 1. ìƒˆ ë…¸ë“œ ë“±ë¡\n",
    "workflow.add_node(\"classify_input\", node_classify_input)\n",
    "workflow.add_node(\"generate_conversational_response\", node_generate_conversational_response)\n",
    "\n",
    "# 2. ê¸°ì¡´ ë…¸ë“œ ë“±ë¡\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "# 3. ì‹œì‘ì  ë³€ê²½\n",
    "workflow.add_edge(START, \"classify_input\")\n",
    "\n",
    "# 4. ì˜ë„ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¶„ê¸°\n",
    "def decide_flow(state: GraphState) -> str:\n",
    "    log_debug(f\"---DECIDING FLOW BASED ON INTENT: {state['intent']}---\")\n",
    "    if state[\"intent\"] == \"conversational\":\n",
    "        return \"generate_conversational_response\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_flow,\n",
    "    {\n",
    "        \"generate_conversational_response\": \"generate_conversational_response\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 5. ê¸°ì¡´ RAG/CRAG íë¦„ ì—°ê²°\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"notify_user\": \"notify_user\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "\n",
    "# 6. ì¢…ë£Œì  ì—°ê²°\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"generate_conversational_response\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI ë° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\n",
    "# --------------------------\n",
    "def get_pdf_list():\n",
    "    \"\"\"'data' ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    return [f.name for f in pathlib.Path(DATA_DIR).glob(\"*.pdf\")]\n",
    "\n",
    "def handle_file_upload(file):\n",
    "    \"\"\"íŒŒì¼ ì—…ë¡œë“œ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤.\"\"\"\n",
    "    if file is None:\n",
    "        return gr.update(choices=get_pdf_list())\n",
    "    \n",
    "    dest_path = pathlib.Path(DATA_DIR) / pathlib.Path(file.name).name\n",
    "    shutil.copy(file.name, dest_path)\n",
    "    \n",
    "    # ìºì‹œì—ì„œ í•´ë‹¹ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œí•˜ì—¬ ë¦¬ë¡œë“œë¥¼ ê°•ì œ\n",
    "    if dest_path.name in retriever_cache:\n",
    "        del retriever_cache[dest_path.name]\n",
    "        \n",
    "    return gr.update(choices=get_pdf_list(), value=dest_path.name)\n",
    "\n",
    "def handle_pdf_selection(pdf_filename, progress=gr.Progress()):\n",
    "    \"\"\"ë“œë¡­ë‹¤ìš´ì—ì„œ PDFë¥¼ ì„ íƒí–ˆì„ ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.\"\"\"\n",
    "    if not pdf_filename:\n",
    "        return \"ë¶„ì„í•  PDF íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\", \"\"\n",
    "\n",
    "    progress(0, desc=\"ë¬¸ì„œ ì²˜ë¦¬ ì¤€ë¹„ ì¤‘...\")\n",
    "    retriever, msg = get_retriever_for_pdf(pdf_filename)\n",
    "    progress(1, desc=msg)\n",
    "    \n",
    "    return msg, pdf_filename\n",
    "\n",
    "def run_crag(query: str, history: List[dict], selected_pdf: str, show_debug: bool):\n",
    "    \"\"\"\n",
    "    ë©”ì¸ CRAG ì‹¤í–‰ í•¨ìˆ˜. ì±„íŒ… ë©”ì‹œì§€ ì œì¶œ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤.\n",
    "    UIì˜ ëª¨ë“  ì…ë ¥ì„ ë°›ì•„ LangGraphë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    global debug_logs\n",
    "    debug_logs = []\n",
    "\n",
    "    # --- ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬ ---\n",
    "    if not query:\n",
    "        history.append({\"role\": \"user\", \"content\": \"\"})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\"})\n",
    "        yield \"\", history, \"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\", \"\"\n",
    "        return\n",
    "\n",
    "    if not selected_pdf:\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"ë¨¼ì € ë¶„ì„í•  PDF ë¬¸ì„œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\"})\n",
    "        yield \"\", history, \"PDF ë¬¸ì„œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\", \"\"\n",
    "        return\n",
    "\n",
    "    # --- Retriever ì¤€ë¹„ ---\n",
    "    retriever, msg = get_retriever_for_pdf(selected_pdf)\n",
    "    if not retriever:\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": f\"ë¬¸ì„œ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg}\"})\n",
    "        yield \"\", history, msg, \"\"\n",
    "        return\n",
    "\n",
    "    # --- LangGraph ì‹¤í–‰ ---\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    \n",
    "    try:\n",
    "        inputs = {\n",
    "            \"question\": query, \n",
    "            \"chat_history\": chat_history_for_chain,\n",
    "            \"retriever\": retriever\n",
    "        }\n",
    "        \n",
    "        # ì‚¬ìš©ì ì§ˆë¬¸ì„ íˆìŠ¤í† ë¦¬ì— ë¨¼ì € ì¶”ê°€\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë° ë‹µë³€ ìƒì„±\n",
    "        generation = \"\"\n",
    "        final_state = {}\n",
    "        for step in app.stream(inputs):\n",
    "            node_name = list(step.keys())[0]\n",
    "            final_state = step[node_name]\n",
    "            log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            \n",
    "            # ì›¹ ê²€ìƒ‰ ì‹œ ì¤‘ê°„ ì•Œë¦¼\n",
    "            if node_name == \"grade_documents\" and final_state.get(\"web_search\") == \"Yes\":\n",
    "                history.append({\"role\": \"assistant\", \"content\": \"ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.\"})\n",
    "                yield \"\", history, \"ì›¹ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...\", \"\"\n",
    "\n",
    "            if \"generation\" in final_state and final_state[\"generation\"]:\n",
    "                 generation = final_state[\"generation\"]\n",
    "\n",
    "        # ìµœì¢… ë‹µë³€ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€/ì—…ë°ì´íŠ¸\n",
    "        if history[-1][\"role\"] == \"assistant\": # ì›¹ ê²€ìƒ‰ ì•Œë¦¼ì´ ìˆì—ˆë˜ ê²½ìš°\n",
    "            history[-1][\"content\"] = generation\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": generation})\n",
    "\n",
    "        # --- ê²°ê³¼ í‘œì‹œ ---\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## ì°¸ì¡° ë¬¸ì„œ\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### ë¬¸ì„œ {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        \n",
    "        yield \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"ì˜¤ë¥˜ ë°œìƒ: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### ì˜¤ë¥˜\\n```\\n\" + err + \"\\n```\"\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"})\n",
    "        yield \"\", history, \"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\", debug_output\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI êµ¬ì„±\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 ì‹œë¦¬ì¦ˆì˜ ì‘ì€ ëª¨ë¸ë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‚˜ìš”?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"Dynamic PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# Dynamic PDF RAG + CRAG Chatbot\")\n",
    "    gr.Markdown(\"ì¢Œì¸¡ ìƒë‹¨ì—ì„œ ë¶„ì„í•  PDFë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. ë¬¸ì„œê°€ ì¤€ë¹„ë˜ë©´ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # í˜„ì¬ ì„ íƒëœ PDF íŒŒì¼ëª…ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìƒíƒœ\n",
    "    selected_pdf_state = gr.State()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # --- íŒŒì¼ ê´€ë¦¬ ì„¹ì…˜ ---\n",
    "            with gr.Accordion(\"1. ë¬¸ì„œ ì„ íƒ ë° ê´€ë¦¬\", open=True):\n",
    "                pdf_selector = gr.Dropdown(\n",
    "                    label=\"ë¶„ì„í•  PDF ë¬¸ì„œ ì„ íƒ\",\n",
    "                    choices=get_pdf_list(),\n",
    "                    interactive=True\n",
    "                )\n",
    "                upload_button = gr.UploadButton(\"PDF ì—…ë¡œë“œ\", file_types=[\".pdf\"])\n",
    "                status_display = gr.Markdown(\"ëŒ€ê¸° ì¤‘...\")\n",
    "\n",
    "            # --- ì±„íŒ… ì„¹ì…˜ ---\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)\")\n",
    "            \n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"ì˜ˆì‹œ ì§ˆë¬¸\"\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM ì°¸ì¡° ë¬¸ì„œ\")\n",
    "            with gr.Accordion(\"âš™ï¸ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")\n",
    "\n",
    "    # --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë°”ì¸ë”© ---\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel, status_display])\n",
    "\n",
    "    # 1. íŒŒì¼ ì—…ë¡œë“œ ì‹œ: íŒŒì¼ì„ ì„œë²„ì— ì €ì¥í•˜ê³ , ë“œë¡­ë‹¤ìš´ ëª©ë¡ì„ ê°±ì‹ \n",
    "    upload_button.upload(handle_file_upload, inputs=[upload_button], outputs=[pdf_selector])\n",
    "\n",
    "    # 2. ë“œë¡­ë‹¤ìš´ì—ì„œ PDF ì„ íƒ ì‹œ: í•´ë‹¹ PDFì— ëŒ€í•œ retrieverë¥¼ ì¤€ë¹„/ë¡œë“œ\n",
    "    pdf_selector.change(\n",
    "        handle_pdf_selection, \n",
    "        inputs=[pdf_selector], \n",
    "        outputs=[status_display, selected_pdf_state]\n",
    "    )\n",
    "\n",
    "    # 3. ë©”ì‹œì§€ ì „ì†¡ ì‹œ: CRAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    msg.submit(\n",
    "        run_crag, \n",
    "        [msg, chatbot, selected_pdf_state, show_debug_checkbox],\n",
    "        [msg, chatbot, context_display, debug_panel]\n",
    "    )\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bd54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
