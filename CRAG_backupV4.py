# =================================================================================================
# íŒŒì¼ ì„¤ëª…: CRAGv4.py
#
# ê¸°ëŠ¥:
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PDF ë¬¸ì„œ ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡ì„ Gradio ì›¹ UIë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.
# LangChainê³¼ LangGraphë¥¼ ê¸°ë°˜ìœ¼ë¡œ Corrective RAG (CRAG) ë° ì¼ë°˜ RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•˜ì—¬,
# ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì‘ë‹µ ë°©ì‹ì„ ë³€ê²½í•˜ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
#
# ì£¼ìš” ì•„í‚¤í…ì²˜ ë° ê¸°ëŠ¥:
# 1.  **ë¬¸ì„œ ì²˜ë¦¬**:
#     - LlamaParseë¥¼ ì‚¬ìš©í•´ PDF ë¬¸ì„œë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
#     - ì²˜ë¦¬ëœ Markdownì€ ì´í›„ ì‹¤í–‰ ì‹œ ì¬ì‚¬ìš©ì„ ìœ„í•´ ìºì‹±ë©ë‹ˆë‹¤.
#
# 2.  **RAG (Retrieval-Augmented Generation)**:
#     - `ParentDocumentRetriever`: ë¬¸ì„œë¥¼ ë¶€ëª¨/ìì‹ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.
#     - `ChromaDB`: ë²¡í„° ì €ì¥ì†Œë¡œ ì‚¬ìš©ë˜ë©°, ì„ë² ë”©ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
#     - `History-Aware Retriever`: ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ì—¬ í›„ì† ì§ˆë¬¸(ì˜ˆ: "ê·¸ê±´ ì–´ë•Œ?")ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
#
# 3.  **CRAG (Corrective RAG) & ë¼ìš°íŒ… (LangGraph ê¸°ë°˜)**:
#     - **ì˜ë„ ë¶„ë¥˜ (Intent Classification)**: ì‚¬ìš©ìì˜ ì…ë ¥ì„ 'ë‹¨ìˆœ ëŒ€í™”'ì™€ 'ì •ë³´ì„± ì§ˆë¬¸'ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°íŒ… ë…¸ë“œë¥¼ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.
#     - **ëŒ€í™”í˜• ì‘ë‹µ**: 'ë‹¨ìˆœ ëŒ€í™”'ë¡œ ë¶„ë¥˜ë˜ë©´, RAG íŒŒì´í”„ë¼ì¸ì„ ê±´ë„ˆë›°ê³  LLMì´ ì§ì ‘ ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
#     - **ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€**: 'ì •ë³´ì„± ì§ˆë¬¸'ì˜ ê²½ìš°, ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ LLMì´ í‰ê°€í•©ë‹ˆë‹¤.
#     - **ì›¹ ê²€ìƒ‰ ë³´ê°•**: ê´€ë ¨ ë¬¸ì„œê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´, ì§ˆë¬¸ì„ ì›¹ ê²€ìƒ‰ì— ë” ì í•©í•˜ê²Œ ë³€í˜•í•œ í›„ Tavily APIë¥¼ í†µí•´ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
#
# 4.  **ë©”ëª¨ë¦¬ ê´€ë¦¬**:
#     - Stateless êµ¬ì¡°: ì„œë²„ëŠ” ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë©°, ë§¤ ìš”ì²­ë§ˆë‹¤ Gradio UI(ë¸Œë¼ìš°ì €)ë¡œë¶€í„° ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ì „ë‹¬ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     - UI ì•Œë¦¼ ë²„ê·¸ ìˆ˜ì •: ì›¹ ê²€ìƒ‰ ì‹œ 'ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤'ì™€ ê°™ì€ ì¤‘ê°„ ê³¼ì •ì˜ ì•Œë¦¼ì´ UIì— ì •ìƒì ìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ `run_crag` í•¨ìˆ˜ì˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë¡œì§ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
#
# 5.  **UI**:
#     - Gradioë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì‰½ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
# =================================================================================================
# ================================================================
# PDF Conversational RAG + CRAG(Conditional RAG) í†µí•© ë²„ì „ (Gradio UI)
# - PDF â†’ LlamaParse(md) â†’ Chroma(Parent/Child) â†’ History-Aware Retrieve
# - CRAG: grade_documents â†’ (generate | transform_query â†’ web_search â†’ generate)
# - ë¬¸ì„œ ì™¸ ì§€ì‹ ê¸ˆì§€, ì—†ìœ¼ë©´ í•œêµ­ì–´ë¡œ "ì œê³µëœ ë¬¸ì„œ..." ì¶œë ¥
# - ì›¹ê²€ìƒ‰: Tavily(ì„ íƒ, ë¯¸ì„¤ì • ì‹œ ìš°íšŒ)
# ================================================================

import os
import json
import traceback
import gradio as gr
from dotenv import load_dotenv

# Python typing
from typing import Iterable, Optional, Tuple, List
from typing_extensions import TypedDict

# PDF Parser
from llama_parse import LlamaParse

# LangChain Core / OpenAI / Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.retrievers import ParentDocumentRetriever
from langchain.schema import Document
from langchain.chains import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain

# LangChain Core (Prompts, Messages, Output parsing)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.stores import BaseStore  # âœ… BaseStoreëŠ” ì—¬ê¸°ë¡œ ì´ë™ë¨
from langchain_core.messages import BaseMessage

# LangGraph
from langgraph.graph import END, START, StateGraph

# Pydantic (v2)
from pydantic import BaseModel, Field

# (Optional) ì›¹ ê²€ìƒ‰ íˆ´
from langchain_tavily import TavilySearch


# --- Add: Persistent JSON-backed DocStore for parents ---

# --------------------------
# ìœ í‹¸: Gradioìš© íˆìŠ¤í† ë¦¬ ë³€í™˜
# --------------------------
def to_lc_messages(history: List[dict]) -> List:
    msgs = []
    for m in history:
        if m["role"] == "user":
            msgs.append(HumanMessage(content=m["content"]))
        elif m["role"] == "assistant":
            msgs.append(AIMessage(content=m["content"]))
    return msgs


def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:
    history = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            history.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            history.append({"role": "assistant", "content": msg.content})
    return history


# --------------------------
# ì „ì—­ Debug Log ì €ì¥ì†Œ
# --------------------------
debug_logs = []

def log_debug(msg: str):
    debug_logs.append(msg)
    print(msg)  # ì½˜ì†”ì—ë„ ê·¸ëŒ€ë¡œ ì°ì–´ì¤Œ

class JSONDocStore(BaseStore[str, Document]):
    """
    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore.
    - key -> ./parent_store/{key}.json ì— Document ì €ì¥
    - ParentDocumentRetriever ê°€ ìš”êµ¬í•˜ëŠ” mset/mget/mdelete/yield_keys êµ¬í˜„
    """
    def __init__(self, root_dir: str = "./parent_store"):
        self.root_dir = root_dir
        os.makedirs(self.root_dir, exist_ok=True)

    def _path(self, key: str) -> str:
        return os.path.join(self.root_dir, f"{key}.json")

    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:
        for key, doc in key_value_pairs:
            with open(self._path(key), "w", encoding="utf-8") as f:
                json.dump(
                    {"page_content": doc.page_content, "metadata": doc.metadata},
                    f,
                    ensure_ascii=False,
                )

    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:
        results: List[Optional[Document]] = []
        for key in keys:
            p = self._path(key)
            if os.path.exists(p):
                with open(p, "r", encoding="utf-8") as f:
                    data = json.load(f)
                results.append(
                    Document(
                        page_content=data.get("page_content", ""),
                        metadata=data.get("metadata", {}),
                    )
                )
            else:
                results.append(None)
        return results

    def mdelete(self, keys: Iterable[str]) -> None:
        for key in keys:
            p = self._path(key)
            if os.path.exists(p):
                os.remove(p)

    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:
        for fname in os.listdir(self.root_dir):
            if not fname.endswith(".json"):
                continue
            key = fname[:-5]  # strip .json
            if prefix is None or key.startswith(prefix):
                yield key


# --------------------------
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ & ì„¤ì •
# --------------------------
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
LLAMA_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
TAVILY_KEY = os.getenv("TAVILY_API_KEY")  # ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ê±´ë„ˆëœ€

# --------------------------
# ê²½ë¡œ ë° ì „ì—­ ì„¤ì •
# --------------------------
PDF_PATH = "data/gemini-2.5-tech_1-10.pdf"
PARSED_MD_PATH = "loaddata/llamaparse_output_gemini_1-10.md"
CHROMA_DB_DIR = "./chroma_db10"

# --------------------------
# LLM & ì„ë² ë”©
# --------------------------
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# --------------------------
# Text Splitters
# --------------------------
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)

# --------------------------
# Vector Store (Chroma)
# --------------------------
vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)

# --------------------------
# ParentDocumentRetriever
# --------------------------
# ê¸°ì¡´: store = InMemoryStore()
store = JSONDocStore("./parent_store")  # íŒŒì¼ ê¸°ë°˜ parent ì €ì¥

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
    search_kwargs={"k": 2},
)


# --------------------------
# ë°ì´í„° ë¡œë”© & ë²¡í„°DB ì ì¬
# --------------------------
def _vs_count_safe() -> int:
    # ë‚´ë¶€ ì†ì„± ì˜ì¡´ì„ ìµœì†Œí™”í•˜ëŠ” ì•ˆì „í•œ ì¹´ìš´íŠ¸ í•¨ìˆ˜
    try:
        return vectorstore._collection.count()  # chroma ë‚´ë¶€
    except Exception:
        try:
            # ê°„ë‹¨íˆ ë¹„ìŠ·ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (ë¹„ì–´ìˆìœ¼ë©´ ì˜ˆì™¸ or ë¹ˆ ê²°ê³¼)
            _ = vectorstore.similarity_search("dummy", k=1)
            # ChromaëŠ” ë¹„ì–´ìˆì–´ë„ í˜¸ì¶œì´ ì„±ê³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ peek ì¨ë´„
            return len(vectorstore._collection.peek()["ids"])  # type: ignore
        except Exception:
            return 0

def load_and_populate_vectorstore():
    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)

    if _vs_count_safe() > 0:
        print(f"[INFO] Vector store already populated. Count={_vs_count_safe()}")
        return

    # MD íŒŒì¼ ì—†ìœ¼ë©´ PDF â†’ LlamaParse â†’ md ì €ì¥
    if not os.path.exists(PARSED_MD_PATH):
        print(f"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...")
        if not LLAMA_KEY:
            raise RuntimeError("LLAMA_CLOUD_API_KEYê°€ ì—†ì–´ PDF íŒŒì‹±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        try:
            parser = LlamaParse(result_type="markdown", api_key=LLAMA_KEY)
            documents = parser.load_data(PDF_PATH)
            md_text = "\n".join([doc.text for doc in documents])
            with open(PARSED_MD_PATH, "w", encoding="utf-8") as f:
                f.write(md_text)
            print(f"[INFO] Parsed & saved to '{PARSED_MD_PATH}'")
        except Exception as e:
            raise RuntimeError(f"LlamaParse ì˜¤ë¥˜: {e}")

    # md ë¡œë“œ â†’ Parent retrieverì— ì¶”ê°€
    print(f"[INFO] Loading markdown from '{PARSED_MD_PATH}'...")
    with open(PARSED_MD_PATH, "r", encoding="utf-8") as f:
        text = f.read()

    # í•˜ë‚˜ì˜ ê±°ëŒ€ ë¬¸ì„œë¡œ ì¶”ê°€ â†’ Parent/Child splitterê°€ ë‚´ë¶€ì—ì„œ ì˜ê²Œ ìª¼ê°¬
    documents = [Document(page_content=text, metadata={"source": PARSED_MD_PATH})]
    retriever.add_documents(documents)
    print(f"[INFO] Vector store populated. Count={_vs_count_safe()}")


# --------------------------
# History-Aware Retriever
# --------------------------
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question which might reference context in the chat history, "
    "formulate a standalone question which can be understood without the chat history. "
    "Do NOT answer the question, just reformulate it if needed and otherwise return it as is."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

# --------------------------
# ìµœì¢… ë‹µë³€(ë¬¸ì„œ ê¸°ë°˜ë§Œ í—ˆìš©) Chain
# --------------------------
ga_system_prompt = (
    "You are a helpful assistant. Your task is to answer the user's question based on the provided context. "
    "The context may come from PDF documents or from web search results. "
    "If useful information is present in the context (including web search), provide a concise answer. "
    "IMPORTANT: You must answer in Korean."
    "\n\nContext:\n{context}"
)

ga_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", ga_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)

# --------------------------
# CRAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(Structured Output)
# --------------------------
class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""
    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")

grade_system_prompt = """You are a grader assessing relevance of a retrieved document to a user question.
If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.
Return 'yes' or 'no'."""
grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", grade_system_prompt),
        ("human", "Retrieved document:\n\n{document}\n\nUser question: {question}"),
    ]
)
structured_llm_grader = llm.with_structured_output(GradeDocuments)
retrieval_grader = grade_prompt | structured_llm_grader

# --------------------------
# CRAG: ì§ˆë¬¸ ì¬ì‘ì„± (ì›¹ê²€ìƒ‰ ì¹œí™”ì )
# --------------------------
rewrite_system = (
    "You are a question re-writer that converts an input question to a better version optimized for web search. "
    "Reason about the underlying semantic intent and produce a clearer query."
)
rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", rewrite_system),
        ("human", "Here is the initial question:\n\n{question}\n\nFormulate an improved question."),
    ]
)
question_rewriter = rewrite_prompt | llm | StrOutputParser()

# --------------------------
# (ì„ íƒ) ì›¹ê²€ìƒ‰ ë„êµ¬
# --------------------------
web_search_tool: Optional[TavilySearch] = None
if TAVILY_KEY:
    web_search_tool = TavilySearch(k=3)
else:
    print("[WARN] TAVILY_API_KEY ë¯¸ì„¤ì •: ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ìƒëµë©ë‹ˆë‹¤.")


# --------------------------
# LangGraph ìƒíƒœ ì •ì˜
# --------------------------
class GraphState(TypedDict):
    question: str
    generation: str
    web_search: str
    documents: List[Document]
    chat_history: List[BaseMessage]
    intent: str  # "conversational" or "question"


# --------------------------------------
# ìƒˆ ë…¸ë“œ/Chain: ì…ë ¥ ì˜ë„ ë¶„ë¥˜
# --------------------------------------
class ClassifyIntent(BaseModel):
    """"conversational" ë˜ëŠ” "question"ìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    intent: str = Field(description="ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„. 'conversational' ë˜ëŠ” 'question' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

classify_system_prompt = """You are a router that classifies the user's input intent. Based on the user's latest message and the previous conversation history, determine if the input is a simple conversation/chit-chat or a question that requires information.
- General greetings like "Hello", "Thank you", "Have a nice day" are 'conversational'.
- Responses to previous answers (e.g., "That's interesting", "I see") are also 'conversational'.
- If the input requires finding information from a PDF document or the web, it is a 'question'.
- If in doubt, classify it as 'question'."""

classify_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", classify_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)
structured_llm_classifier = llm.with_structured_output(ClassifyIntent)
intent_classifier = classify_prompt | structured_llm_classifier


def node_classify_input(state: GraphState) -> GraphState:
    """ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì—¬ stateì— ì €ì¥"""
    log_debug("---CLASSIFYING INPUT INTENT---")
    intent_result = intent_classifier.invoke({
        "question": state["question"],
        "chat_history": state.get("chat_history", [])
    })
    log_debug(f"  [Intent] Classified as: {intent_result.intent}")
    return {"intent": intent_result.intent}


# --------------------------------------
# ìƒˆ ë…¸ë“œ/Chain: ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ ìƒì„±
# --------------------------------------
conv_gen_system_prompt = """You are a friendly AI assistant. Respond to the user's message in a natural, conversational way. Do not search for information; just generate a simple response that fits the context of the conversation. IMPORTANT: You must answer in Korean."""

conv_gen_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", conv_gen_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)
conversational_chain = conv_gen_prompt | llm | StrOutputParser()


def node_generate_conversational_response(state: GraphState) -> GraphState:
    """ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±"""
    log_debug("---GENERATING CONVERSATIONAL RESPONSE---")
    generation = conversational_chain.invoke({
        "input": state["question"],
        "chat_history": state.get("chat_history", [])
    })
    return {"generation": generation}


# --------------------------
# LangGraph ë…¸ë“œ í•¨ìˆ˜
# --------------------------
def node_retrieve(state: GraphState) -> GraphState:
    log_debug("---RETRIEVE---")

    question = state["question"]
    chat_history = state.get("chat_history", [])

    # ì› ì§ˆë¬¸ + íˆìŠ¤í† ë¦¬ ì¶œë ¥
    log_debug(f"[DEBUG] Raw Question: {question}")
    if chat_history:
        log_debug(f"[DEBUG] Chat History Count: {len(chat_history)}")
    else:
        log_debug("[DEBUG] No chat history provided.")

    # Child ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    child_results = vectorstore.similarity_search(question, k=2)
    log_debug("=== Child ê²€ìƒ‰ ê²°ê³¼ ===")
    for i, d in enumerate(child_results, 1):
        log_debug(f"[Child {i}] Parent ID: {d.metadata.get('doc_id')}")
        log_debug(f"Snippet: {d.page_content[:200]}...\n")

    # Parent ë³µêµ¬ ê²°ê³¼ (History-aware retriever ì‚¬ìš©)
    docs = history_aware_retriever.invoke(
        {"input": question, "chat_history": chat_history}
    )
    log_debug("=== Parent ë³µêµ¬ ê²°ê³¼ ===")
    for i, d in enumerate(docs, 1):
        log_debug(f"[Parent {i}] Source: {d.metadata.get('source', 'N/A')}")
        log_debug(f"Snippet: {d.page_content[:500]}...\n")

    return {
        "documents": docs,
        "question": question,
        "chat_history": chat_history,
        "web_search": "No",
        "generation": "",
    }


def node_grade_documents(state: GraphState) -> GraphState:
    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    filtered_docs: List[Document] = []
    for d in documents:
        try:
            score = retrieval_grader.invoke({"question": question, "document": d.page_content})
            if score.binary_score.strip().lower() == "yes":
                print("---GRADE: DOCUMENT RELEVANT---")
                filtered_docs.append(d)
            else:
                print("---GRADE: DOCUMENT NOT RELEVANT---")
        except Exception:
            # ê·¸ë ˆì´ë” ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€
            filtered_docs.append(d)

    web_search_flag = "Yes" if len(filtered_docs) == 0 else "No"
    return {
        "documents": filtered_docs,
        "question": question,
        "web_search": web_search_flag,
        "chat_history": state["chat_history"],
        "generation": state.get("generation", ""),
    }

def node_decide_to_generate(state: GraphState) -> str:
    print("---ASSESS GRADED DOCUMENTS---")
    return "notify_user" if state["web_search"] == "Yes" else "generate"

def node_transform_query(state: GraphState) -> GraphState:
    print("---TRANSFORM QUERY---")
    better_question = question_rewriter.invoke({"question": state["question"]})
    return {
        "documents": state["documents"],
        "question": better_question,
        "web_search": state["web_search"],
        "chat_history": state["chat_history"],
        "generation": state.get("generation", ""),
    }

def node_web_search(state: GraphState) -> GraphState:
    print("---WEB SEARCH---")
    documents = state["documents"]
    question = state["question"]

    if web_search_tool is None:
        web_results_text = "ì›¹ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    else:
        try:
            results = web_search_tool.invoke({"query": question})

            # âœ… ê²°ê³¼ê°€ ë¬¸ìì—´ì¼ ë•Œ ëŒ€ë¹„
            if isinstance(results, str):
                web_results_text = results
            elif isinstance(results, list):
                lines = []
                for r in results:
                    if isinstance(r, dict):  # dict íƒ€ì…ë§Œ ì²˜ë¦¬
                        title = r.get("title", "")
                        url = r.get("url", "")
                        content = r.get("content", "")
                        lines.append(f"[{title}] {url}\n{content}\n")
                    else:
                        lines.append(str(r))  # dictê°€ ì•„ë‹ˆë©´ ë¬¸ìì—´ ë³€í™˜
                web_results_text = "\n---\n".join(lines) if lines else "ê²€ìƒ‰ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
            else:
                web_results_text = str(results)

        except Exception as e:
            web_results_text = f"ì›¹ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}"

    documents = documents + [Document(page_content=web_results_text, metadata={"source": "tavily"})]
    return {
        "documents": documents,
        "question": question,
        "web_search": "No",
        "chat_history": state["chat_history"],
        "generation": state.get("generation", ""),
    }


def node_generate(state: GraphState) -> GraphState:
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]
    chat_history = state["chat_history"]

    # ë‹µë³€ ìƒì„±
    answer = question_answer_chain.invoke({
        "input": question,
        "chat_history": chat_history,
        "context": documents
    })

    # ì¶œì²˜ êµ¬ë¶„
    if any(d.metadata.get("source") == "tavily" for d in documents):
        source_tag = "\n\n[ì¶œì²˜: ì›¹ê²€ìƒ‰ ê²°ê³¼]"
    else:
        source_tag = "\n\n[ì¶œì²˜: PDF ë¬¸ì„œ]"

    return {
        "documents": documents,
        "question": question,
        "web_search": "No",
        "chat_history": chat_history,
        "generation": (answer if isinstance(answer, str) else str(answer)) + source_tag,
    }


# --------------------------
# ìƒˆ ë…¸ë“œ: ì‚¬ìš©ì ì•Œë¦¼
# --------------------------
def node_notify_user(state: GraphState) -> GraphState:
    log_debug("---NOTIFY USER---")
    notice = "ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤."
    chat_history = state.get("chat_history", [])
    chat_history.append(AIMessage(content=notice))  # âœ… dictë¡œë§Œ ìœ ì§€
    return {**state, "chat_history": chat_history}



# --------------------------
# ê·¸ë˜í”„ êµ¬ì„± & ì»´íŒŒì¼
# --------------------------
workflow = StateGraph(GraphState)

# 1. ìƒˆ ë…¸ë“œ ë“±ë¡
workflow.add_node("classify_input", node_classify_input)
workflow.add_node("generate_conversational_response", node_generate_conversational_response)

# 2. ê¸°ì¡´ ë…¸ë“œ ë“±ë¡
workflow.add_node("retrieve", node_retrieve)
workflow.add_node("grade_documents", node_grade_documents)
workflow.add_node("generate", node_generate)
workflow.add_node("notify_user", node_notify_user)
workflow.add_node("transform_query", node_transform_query)
workflow.add_node("web_search_node", node_web_search)

# 3. ì‹œì‘ì  ë³€ê²½
workflow.add_edge(START, "classify_input")

# 4. ì˜ë„ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¶„ê¸°
def decide_flow(state: GraphState) -> str:
    log_debug(f"---DECIDING FLOW BASED ON INTENT: {state['intent']}---")
    if state["intent"] == "conversational":
        return "generate_conversational_response"
    else:
        return "retrieve"

workflow.add_conditional_edges(
    "classify_input",
    decide_flow,
    {
        "generate_conversational_response": "generate_conversational_response",
        "retrieve": "retrieve",
    },
)

# 5. ê¸°ì¡´ RAG/CRAG íë¦„ ì—°ê²°
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    node_decide_to_generate,
    {
        "generate": "generate",
        "notify_user": "notify_user",
    },
)
workflow.add_edge("notify_user", "transform_query")
workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")

# 6. ì¢…ë£Œì  ì—°ê²°
workflow.add_edge("generate", END)
workflow.add_edge("generate_conversational_response", END)

app = workflow.compile()




# --------------------------
# run_crag ìˆ˜ì •
# --------------------------
def run_crag(query: str, history: List[dict], show_debug: bool):
    global debug_logs
    debug_logs = []  # ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°í™”

    chat_history_for_chain = to_lc_messages(history or [])
    try:
        final_state = None
        inputs = {"question": query, "chat_history": chat_history_for_chain,
                  "documents": [], "web_search": "No", "generation": ""}
        for step in app.stream(inputs):
            for node_name, node_state in step.items():
                log_debug(f"[TRACE] Node '{node_name}' passed.")
            final_state = node_state

        # ìµœì¢… ì‘ë‹µ
        answer = final_state.get("generation", "ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        docs: List[Document] = final_state.get("documents", [])
        context_md = "## ì°¸ì¡° ë¬¸ì„œ\n\n"
        if docs:
            for i, d in enumerate(docs, 1):
                src = d.metadata.get("source", "N/A")
                snippet = d.page_content[:500] + ("..." if len(d.page_content) > 500 else "")
                context_md += f"### ë¬¸ì„œ {i} (source: {src})\n```\n{snippet}\n```\n\n"
        else:
            context_md += "ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìˆ˜ì •ëœ ë¡œì§)
        # ê·¸ë˜í”„ ì‹¤í–‰ í›„ì˜ ìµœì¢… ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜´ (ì—¬ê¸°ì—” notify ë©”ì‹œì§€ ë“±ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ)
        final_lc_history = final_state.get("chat_history", chat_history_for_chain)
        history = to_gradio_history(final_lc_history)

        # í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìµœì¢… ë‹µë³€ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": answer})

        # ë””ë²„ê·¸ í‘œì‹œ ì—¬ë¶€ ê²°ì •
        debug_output = "### Debug Logs\n```\n" + "\n".join(debug_logs) + "\n```" if show_debug else ""
        return "", history, context_md, debug_output

    except Exception as e:
        err = f"ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}"
        debug_output = "### ì˜¤ë¥˜\n```\n" + err + "\n```"
        return "", history, "ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.", debug_output



def force_reload_vectorstore():
    try:
        print("[INFO] Resetting Chroma client...")
        vectorstore._client.reset()  # ì „ì²´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        load_and_populate_vectorstore()
        return "âœ… Vector store reloaded successfully!"
    except Exception as e:
        return f"âŒ Error during vector store reload: {e}"


# --------------------------
# ì´ˆê¸° ì ì¬
# --------------------------
load_and_populate_vectorstore()

# --------------------------
# Gradio UI
# --------------------------
example_questions = [
    "Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?",
    "Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?",
    "Gemini 2.5 ì‹œë¦¬ì¦ˆì˜ ì‘ì€ ëª¨ë¸ë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‚˜ìš”?",
]

with gr.Blocks(theme="soft", title="PDF RAG + CRAG Chatbot") as demo:
    gr.Markdown("# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)")
    gr.Markdown("PDF ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. ë¬¸ì„œì—ì„œ ëª» ì°¾ìœ¼ë©´ ì§ˆë¬¸ ì¬ì‘ì„± + (ì„ íƒ)ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.")

    with gr.Row():
        # ------------------------------
        # ì™¼ìª½: ì±„íŒ… ì˜ì—­
        # ------------------------------
        with gr.Column(scale=1):
            chatbot = gr.Chatbot(height=420, label="Chat", type="messages", value=[])
            msg = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)")

            gr.Examples(
                examples=example_questions,
                inputs=msg,
                label="ì˜ˆì‹œ ì§ˆë¬¸"
            )

        # ------------------------------
        # ì˜¤ë¥¸ìª½: ë¬¸ì„œ/ì˜µì…˜/ë””ë²„ê·¸ ì˜ì—­
        # ------------------------------
        with gr.Column(scale=2):
            context_display = gr.Markdown(label="LLM ì°¸ì¡° ë¬¸ì„œ ì „ë¬¸/ìš”ì•½")

            with gr.Accordion("âš™ï¸ Advanced Options", open=False):
                show_debug_checkbox = gr.Checkbox(label="Show Debug Logs", value=False)
                debug_panel = gr.Markdown(label="Debug Logs")   # âœ… ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ íŒ¨ë„
                reload_button = gr.Button("ğŸ”„ Force Reload Vector Store")
                reload_status = gr.Markdown()

    # ------------------------------
    # ë²„íŠ¼/ì´ë²¤íŠ¸ ë°”ì¸ë”©
    # ------------------------------
    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel])
    msg.submit(run_crag, [msg, chatbot, show_debug_checkbox],
               [msg, chatbot, context_display, debug_panel])
    reload_button.click(force_reload_vectorstore, outputs=reload_status)

demo.launch()