{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83181df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vector store already populated. Count=174\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) í†µí•© ë²„ì „ (Gradio UI)\n",
    "# - PDF â†’ LlamaParse(md) â†’ Chroma(Parent/Child) â†’ History-Aware Retrieve\n",
    "# - CRAG: grade_documents â†’ (generate | transform_query â†’ web_search â†’ generate)\n",
    "# - ë¬¸ì„œ ì™¸ ì§€ì‹ ê¸ˆì§€, ì—†ìœ¼ë©´ í•œêµ­ì–´ë¡œ \"ì œê³µëœ ë¬¸ì„œ...\" ì¶œë ¥\n",
    "# - ì›¹ê²€ìƒ‰: Tavily(ì„ íƒ, ë¯¸ì„¤ì • ì‹œ ìš°íšŒ)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # âœ… BaseStoreëŠ” ì—¬ê¸°ë¡œ ì´ë™ë¨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) ì›¹ ê²€ìƒ‰ íˆ´\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# ìœ í‹¸: Gradioìš© ížˆìŠ¤í† ë¦¬ ë³€í™˜\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:\n",
    "    history = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì „ì—­ Debug Log ì €ìž¥ì†Œ\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # ì½˜ì†”ì—ë„ ê·¸ëŒ€ë¡œ ì°ì–´ì¤Œ\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ DocStore.\n",
    "    - key -> ./parent_store/{key}.json ì— Document ì €ìž¥\n",
    "    - ParentDocumentRetriever ê°€ ìš”êµ¬í•˜ëŠ” mset/mget/mdelete/yield_keys êµ¬í˜„\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ & ì„¤ì •\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # ì—†ìœ¼ë©´ ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "# --------------------------\n",
    "# ê²½ë¡œ ë° ì „ì—­ ì„¤ì •\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & ìž„ë² ë”©\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# ê¸°ì¡´: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # íŒŒì¼ ê¸°ë°˜ parent ì €ìž¥\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ë°ì´í„° ë¡œë”© & ë²¡í„°DB ì ìž¬\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # ë‚´ë¶€ ì†ì„± ì˜ì¡´ì„ ìµœì†Œí™”í•˜ëŠ” ì•ˆì „í•œ ì¹´ìš´íŠ¸ í•¨ìˆ˜\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma ë‚´ë¶€\n",
    "    except Exception:\n",
    "        try:\n",
    "            # ê°„ë‹¨ížˆ ë¹„ìŠ·ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (ë¹„ì–´ìžˆìœ¼ë©´ ì˜ˆì™¸ or ë¹ˆ ê²°ê³¼)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # ChromaëŠ” ë¹„ì–´ìžˆì–´ë„ í˜¸ì¶œì´ ì„±ê³µí•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ peek ì¨ë´„\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD íŒŒì¼ ì—†ìœ¼ë©´ PDF â†’ LlamaParse â†’ md ì €ìž¥\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEYê°€ ì—†ì–´ PDF íŒŒì‹±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    # md ë¡œë“œ â†’ Parent retrieverì— ì¶”ê°€\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # í•˜ë‚˜ì˜ ê±°ëŒ€ ë¬¸ì„œë¡œ ì¶”ê°€ â†’ Parent/Child splitterê°€ ë‚´ë¶€ì—ì„œ ìž˜ê²Œ ìª¼ê°¬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# ìµœì¢… ë‹µë³€(ë¬¸ì„œ ê¸°ë°˜ë§Œ í—ˆìš©) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: ì§ˆë¬¸ ìž¬ìž‘ì„± (ì›¹ê²€ìƒ‰ ì¹œí™”ì )\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (ì„ íƒ) ì›¹ê²€ìƒ‰ ë„êµ¬\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY ë¯¸ì„¤ì •: ì›¹ê²€ìƒ‰ ë³´ê°•ì€ ìƒëžµë©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]\n",
    "    intent: str  # \"conversational\" or \"question\"\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ìƒˆ ë…¸ë“œ/Chain: ìž…ë ¥ ì˜ë„ ë¶„ë¥˜\n",
    "# --------------------------------------\n",
    "class ClassifyIntent(BaseModel):\n",
    "    \"\"\"\"conversational\" ë˜ëŠ” \"question\"ìœ¼ë¡œ ì‚¬ìš©ìž ìž…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    intent: str = Field(description=\"ì‚¬ìš©ìž ìž…ë ¥ì˜ ì˜ë„. 'conversational' ë˜ëŠ” 'question' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "classify_system_prompt = \"\"\"You are a router that classifies the user's input intent. Based on the user's latest message and the previous conversation history, determine if the input is a simple conversation/chit-chat or a question that requires information.\n",
    "- General greetings like \"Hello\", \"Thank you\", \"Have a nice day\" are 'conversational'.\n",
    "- Responses to previous answers (e.g., \"That's interesting\", \"I see\") are also 'conversational'.\n",
    "- If the input requires finding information from a PDF document or the web, it is a 'question'.\n",
    "- If in doubt, classify it as 'question'.\"\"\"\n",
    "\n",
    "classify_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", classify_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_classifier = llm.with_structured_output(ClassifyIntent)\n",
    "intent_classifier = classify_prompt | structured_llm_classifier\n",
    "\n",
    "\n",
    "def node_classify_input(state: GraphState) -> GraphState:\n",
    "    \"\"\"ì‚¬ìš©ìž ìž…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì—¬ stateì— ì €ìž¥\"\"\"\n",
    "    log_debug(\"---CLASSIFYING INPUT INTENT---\")\n",
    "    intent_result = intent_classifier.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    log_debug(f\"  [Intent] Classified as: {intent_result.intent}\")\n",
    "    return {\"intent\": intent_result.intent}\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ìƒˆ ë…¸ë“œ/Chain: ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ ìƒì„±\n",
    "# --------------------------------------\n",
    "conv_gen_system_prompt = \"\"\"You are a friendly AI assistant. Respond to the user's message in a natural, conversational way. Do not search for information; just generate a simple response that fits the context of the conversation.\"\"\"\n",
    "\n",
    "conv_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", conv_gen_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "conversational_chain = conv_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def node_generate_conversational_response(state: GraphState) -> GraphState:\n",
    "    \"\"\"ë‹¨ìˆœ ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±\"\"\"\n",
    "    log_debug(\"---GENERATING CONVERSATIONAL RESPONSE---\")\n",
    "    generation = conversational_chain.invoke({\n",
    "        \"input\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph ë…¸ë“œ í•¨ìˆ˜\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # ì› ì§ˆë¬¸ + ížˆìŠ¤í† ë¦¬ ì¶œë ¥\n",
    "    log_debug(f\"[DEBUG] Raw Question: {question}\")\n",
    "    if chat_history:\n",
    "        log_debug(f\"[DEBUG] Chat History Count: {len(chat_history)}\")\n",
    "    else:\n",
    "        log_debug(\"[DEBUG] No chat history provided.\")\n",
    "\n",
    "    # Child ê²€ìƒ‰ ê²°ê³¼ í™•ì¸\n",
    "    child_results = vectorstore.similarity_search(question, k=2)\n",
    "    log_debug(\"=== Child ê²€ìƒ‰ ê²°ê³¼ ===\")\n",
    "    for i, d in enumerate(child_results, 1):\n",
    "        log_debug(f\"[Child {i}] Parent ID: {d.metadata.get('doc_id')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:200]}...\\n\")\n",
    "\n",
    "    # Parent ë³µêµ¬ ê²°ê³¼ (History-aware retriever ì‚¬ìš©)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(\"=== Parent ë³µêµ¬ ê²°ê³¼ ===\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        log_debug(f\"[Parent {i}] Source: {d.metadata.get('source', 'N/A')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:500]}...\\n\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # ê·¸ë ˆì´ë” ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        web_results_text = \"ì›¹ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "            # âœ… ê²°ê³¼ê°€ ë¬¸ìžì—´ì¼ ë•Œ ëŒ€ë¹„\n",
    "            if isinstance(results, str):\n",
    "                web_results_text = results\n",
    "            elif isinstance(results, list):\n",
    "                lines = []\n",
    "                for r in results:\n",
    "                    if isinstance(r, dict):  # dict íƒ€ìž…ë§Œ ì²˜ë¦¬\n",
    "                        title = r.get(\"title\", \"\")\n",
    "                        url = r.get(\"url\", \"\")\n",
    "                        content = r.get(\"content\", \"\")\n",
    "                        lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "                    else:\n",
    "                        lines.append(str(r))  # dictê°€ ì•„ë‹ˆë©´ ë¬¸ìžì—´ ë³€í™˜\n",
    "                web_results_text = \"\\n---\\n\".join(lines) if lines else \"ê²€ìƒ‰ê²°ê³¼ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.\"\n",
    "            else:\n",
    "                web_results_text = str(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"ì›¹ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # ì¶œì²˜ êµ¬ë¶„\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: ì›¹ê²€ìƒ‰ ê²°ê³¼]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[ì¶œì²˜: PDF ë¬¸ì„œ]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ìƒˆ ë…¸ë“œ: ì‚¬ìš©ìž ì•Œë¦¼\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # âœ… dictë¡œë§Œ ìœ ì§€\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ê·¸ëž˜í”„ êµ¬ì„± & ì»´íŒŒì¼\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 1. ìƒˆ ë…¸ë“œ ë“±ë¡\n",
    "workflow.add_node(\"classify_input\", node_classify_input)\n",
    "workflow.add_node(\"generate_conversational_response\", node_generate_conversational_response)\n",
    "\n",
    "# 2. ê¸°ì¡´ ë…¸ë“œ ë“±ë¡\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "# 3. ì‹œìž‘ì  ë³€ê²½\n",
    "workflow.add_edge(START, \"classify_input\")\n",
    "\n",
    "# 4. ì˜ë„ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¶„ê¸°\n",
    "def decide_flow(state: GraphState) -> str:\n",
    "    log_debug(f\"---DECIDING FLOW BASED ON INTENT: {state['intent']}---\")\n",
    "    if state[\"intent\"] == \"conversational\":\n",
    "        return \"generate_conversational_response\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_flow,\n",
    "    {\n",
    "        \"generate_conversational_response\": \"generate_conversational_response\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 5. ê¸°ì¡´ RAG/CRAG íë¦„ ì—°ê²°\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"notify_user\": \"notify_user\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "\n",
    "# 6. ì¢…ë£Œì  ì—°ê²°\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"generate_conversational_response\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# run_crag ìˆ˜ì •\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict], show_debug: bool):\n",
    "    global debug_logs\n",
    "    debug_logs = []  # ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì´ˆê¸°í™”\n",
    "\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain,\n",
    "                  \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            for node_name, node_state in step.items():\n",
    "                log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state\n",
    "\n",
    "        # ìµœì¢… ì‘ë‹µ\n",
    "        answer = final_state.get(\"generation\", \"ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## ì°¸ì¡° ë¬¸ì„œ\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### ë¬¸ì„œ {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        # ížˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìˆ˜ì •ëœ ë¡œì§)\n",
    "        # ê·¸ëž˜í”„ ì‹¤í–‰ í›„ì˜ ìµœì¢… ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜´ (ì—¬ê¸°ì—” notify ë©”ì‹œì§€ ë“±ì´ í¬í•¨ë  ìˆ˜ ìžˆìŒ)\n",
    "        final_lc_history = final_state.get(\"chat_history\", chat_history_for_chain)\n",
    "        history = to_gradio_history(final_lc_history)\n",
    "\n",
    "        # í˜„ìž¬ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ê³¼ ìµœì¢… ë‹µë³€ì„ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        # ë””ë²„ê·¸ í‘œì‹œ ì—¬ë¶€ ê²°ì •\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        return \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"ì˜¤ë¥˜ ë°œìƒ: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### ì˜¤ë¥˜\\n```\\n\" + err + \"\\n```\"\n",
    "        return \"\", history, \"ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\", debug_output\n",
    "\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # ì „ì²´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"âœ… Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# ì´ˆê¸° ì ìž¬\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 ProëŠ” Gemini 1.5 Proì™€ ë¹„êµí–ˆì„ ë•Œ ì–´ë–¤ ì ì—ì„œ í–¥ìƒë˜ì—ˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 Proì™€ FlashëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆë‚˜ìš”?\",\n",
    "    \"Gemini 2.5 ì‹œë¦¬ì¦ˆì˜ ìž‘ì€ ëª¨ë¸ë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‚˜ìš”?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. ë¬¸ì„œì—ì„œ ëª» ì°¾ìœ¼ë©´ ì§ˆë¬¸ ìž¬ìž‘ì„± + (ì„ íƒ)ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ------------------------------\n",
    "        # ì™¼ìª½: ì±„íŒ… ì˜ì—­\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”... (Shift+Enter ì¤„ë°”ê¿ˆ)\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"ì˜ˆì‹œ ì§ˆë¬¸\"\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        # ì˜¤ë¥¸ìª½: ë¬¸ì„œ/ì˜µì…˜/ë””ë²„ê·¸ ì˜ì—­\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM ì°¸ì¡° ë¬¸ì„œ ì „ë¬¸/ìš”ì•½\")\n",
    "\n",
    "            with gr.Accordion(\"âš™ï¸ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")   # âœ… ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ íŒ¨ë„\n",
    "                reload_button = gr.Button(\"ðŸ”„ Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    # ------------------------------\n",
    "    # ë²„íŠ¼/ì´ë²¤íŠ¸ ë°”ì¸ë”©\n",
    "    # ------------------------------\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel])\n",
    "    msg.submit(run_crag, [msg, chatbot, show_debug_checkbox],\n",
    "               [msg, chatbot, context_display, debug_panel])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461fbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê·¸ëž˜í”„ ì‹œê°í™” ì‹¤íŒ¨ (ì¶”ê°€ ì¢…ì†ì„± í•„ìš”): Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "ASCIIë¡œ ê·¸ëž˜í”„ í‘œì‹œ:\n",
      "ASCII í‘œì‹œë„ ì‹¤íŒ¨: Install grandalf to draw graphs: `pip install grandalf`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3c2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (9.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9900d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(\"x\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f45094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement gooogle (from versions: none)\n",
      "ERROR: No matching distribution found for gooogle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.genai\n",
      "  Downloading google_genai-1.38.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google.genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google.genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google.genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google.genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google.genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google.genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google.genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google.genai) (0.6.1)\n",
      "Downloading google_genai-1.38.0-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: google.genai\n",
      "Successfully installed google.genai-1.38.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gooogle\n",
    "!pip install google.genai\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
