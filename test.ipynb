{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83181df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBA\\miniconda3\\envs\\mp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vector store already populated. Count=174\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# PDF Conversational RAG + CRAG(Conditional RAG) 통합 버전 (Gradio UI)\n",
    "# - PDF → LlamaParse(md) → Chroma(Parent/Child) → History-Aware Retrieve\n",
    "# - CRAG: grade_documents → (generate | transform_query → web_search → generate)\n",
    "# - 문서 외 지식 금지, 없으면 한국어로 \"제공된 문서...\" 출력\n",
    "# - 웹검색: Tavily(선택, 미설정 시 우회)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python typing\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# PDF Parser\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "# LangChain Core / OpenAI / Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# LangChain Core (Prompts, Messages, Output parsing)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.stores import BaseStore  # ✅ BaseStore는 여기로 이동됨\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Pydantic (v2)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# (Optional) 웹 검색 툴\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# --- Add: Persistent JSON-backed DocStore for parents ---\n",
    "\n",
    "# --------------------------\n",
    "# 유틸: Gradio용 히스토리 변환\n",
    "# --------------------------\n",
    "def to_lc_messages(history: List[dict]) -> List:\n",
    "    msgs = []\n",
    "    for m in history:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            msgs.append(HumanMessage(content=m[\"content\"]))\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            msgs.append(AIMessage(content=m[\"content\"]))\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def to_gradio_history(messages: List[BaseMessage]) -> List[dict]:\n",
    "    history = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 전역 Debug Log 저장소\n",
    "# --------------------------\n",
    "debug_logs = []\n",
    "\n",
    "def log_debug(msg: str):\n",
    "    debug_logs.append(msg)\n",
    "    print(msg)  # 콘솔에도 그대로 찍어줌\n",
    "\n",
    "class JSONDocStore(BaseStore[str, Document]):\n",
    "    \"\"\"\n",
    "    간단한 파일 기반 영구 DocStore.\n",
    "    - key -> ./parent_store/{key}.json 에 Document 저장\n",
    "    - ParentDocumentRetriever 가 요구하는 mset/mget/mdelete/yield_keys 구현\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str = \"./parent_store\"):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _path(self, key: str) -> str:\n",
    "        return os.path.join(self.root_dir, f\"{key}.json\")\n",
    "\n",
    "    def mset(self, key_value_pairs: Iterable[Tuple[str, Document]]) -> None:\n",
    "        for key, doc in key_value_pairs:\n",
    "            with open(self._path(key), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\"page_content\": doc.page_content, \"metadata\": doc.metadata},\n",
    "                    f,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "\n",
    "    def mget(self, keys: Iterable[str]) -> List[Optional[Document]]:\n",
    "        results: List[Optional[Document]] = []\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(\n",
    "                    Document(\n",
    "                        page_content=data.get(\"page_content\", \"\"),\n",
    "                        metadata=data.get(\"metadata\", {}),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def mdelete(self, keys: Iterable[str]) -> None:\n",
    "        for key in keys:\n",
    "            p = self._path(key)\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    def yield_keys(self, prefix: Optional[str] = None) -> Iterable[str]:\n",
    "        for fname in os.listdir(self.root_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            key = fname[:-5]  # strip .json\n",
    "            if prefix is None or key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 환경변수 로드 & 설정\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")  # 없으면 웹검색 보강은 건너뜀\n",
    "\n",
    "# --------------------------\n",
    "# 경로 및 전역 설정\n",
    "# --------------------------\n",
    "PDF_PATH = \"data/gemini-2.5-tech_1-10.pdf\"\n",
    "PARSED_MD_PATH = \"loaddata/llamaparse_output_gemini_1-10.md\"\n",
    "CHROMA_DB_DIR = \"./chroma_db10\"\n",
    "\n",
    "# --------------------------\n",
    "# LLM & 임베딩\n",
    "# --------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Text Splitters\n",
    "# --------------------------\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# --------------------------\n",
    "# Vector Store (Chroma)\n",
    "# --------------------------\n",
    "vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# ParentDocumentRetriever\n",
    "# --------------------------\n",
    "# 기존: store = InMemoryStore()\n",
    "store = JSONDocStore(\"./parent_store\")  # 파일 기반 parent 저장\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 데이터 로딩 & 벡터DB 적재\n",
    "# --------------------------\n",
    "def _vs_count_safe() -> int:\n",
    "    # 내부 속성 의존을 최소화하는 안전한 카운트 함수\n",
    "    try:\n",
    "        return vectorstore._collection.count()  # chroma 내부\n",
    "    except Exception:\n",
    "        try:\n",
    "            # 간단히 비슷문서 조회 시도 (비어있으면 예외 or 빈 결과)\n",
    "            _ = vectorstore.similarity_search(\"dummy\", k=1)\n",
    "            # Chroma는 비어있어도 호출이 성공할 수 있으므로 peek 써봄\n",
    "            return len(vectorstore._collection.peek()[\"ids\"])  # type: ignore\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "def load_and_populate_vectorstore():\n",
    "    os.makedirs(os.path.dirname(PARSED_MD_PATH), exist_ok=True)\n",
    "    os.makedirs(CHROMA_DB_DIR, exist_ok=True)\n",
    "\n",
    "    if _vs_count_safe() > 0:\n",
    "        print(f\"[INFO] Vector store already populated. Count={_vs_count_safe()}\")\n",
    "        return\n",
    "\n",
    "    # MD 파일 없으면 PDF → LlamaParse → md 저장\n",
    "    if not os.path.exists(PARSED_MD_PATH):\n",
    "        print(f\"[INFO] '{PARSED_MD_PATH}' not found. Parsing PDF with LlamaParse...\")\n",
    "        if not LLAMA_KEY:\n",
    "            raise RuntimeError(\"LLAMA_CLOUD_API_KEY가 없어 PDF 파싱을 수행할 수 없습니다.\")\n",
    "        try:\n",
    "            parser = LlamaParse(result_type=\"markdown\", api_key=LLAMA_KEY)\n",
    "            documents = parser.load_data(PDF_PATH)\n",
    "            md_text = \"\\n\".join([doc.text for doc in documents])\n",
    "            with open(PARSED_MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(md_text)\n",
    "            print(f\"[INFO] Parsed & saved to '{PARSED_MD_PATH}'\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LlamaParse 오류: {e}\")\n",
    "\n",
    "    # md 로드 → Parent retriever에 추가\n",
    "    print(f\"[INFO] Loading markdown from '{PARSED_MD_PATH}'...\")\n",
    "    with open(PARSED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 하나의 거대 문서로 추가 → Parent/Child splitter가 내부에서 잘게 쪼갬\n",
    "    documents = [Document(page_content=text, metadata={\"source\": PARSED_MD_PATH})]\n",
    "    retriever.add_documents(documents)\n",
    "    print(f\"[INFO] Vector store populated. Count={_vs_count_safe()}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# History-Aware Retriever\n",
    "# --------------------------\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# 최종 답변(문서 기반만 허용) Chain\n",
    "# --------------------------\n",
    "ga_system_prompt = (\n",
    "    \"You are a helpful assistant. Your task is to answer the user's question based on the provided context. \"\n",
    "    \"The context may come from PDF documents or from web search results. \"\n",
    "    \"If useful information is present in the context (including web search), provide a concise answer. \"\n",
    "    \"\\n\\nContext:\\n{context}\"\n",
    ")\n",
    "\n",
    "ga_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ga_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 문서 관련성 평가(Structured Output)\n",
    "# --------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return 'yes' or 'no'.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# --------------------------\n",
    "# CRAG: 질문 재작성 (웹검색 친화적)\n",
    "# --------------------------\n",
    "rewrite_system = (\n",
    "    \"You are a question re-writer that converts an input question to a better version optimized for web search. \"\n",
    "    \"Reason about the underlying semantic intent and produce a clearer query.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system),\n",
    "        (\"human\", \"Here is the initial question:\\n\\n{question}\\n\\nFormulate an improved question.\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --------------------------\n",
    "# (선택) 웹검색 도구\n",
    "# --------------------------\n",
    "web_search_tool: Optional[TavilySearch] = None\n",
    "if TAVILY_KEY:\n",
    "    web_search_tool = TavilySearch(k=3)\n",
    "else:\n",
    "    print(\"[WARN] TAVILY_API_KEY 미설정: 웹검색 보강은 생략됩니다.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 상태 정의\n",
    "# --------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[Document]\n",
    "    chat_history: List[BaseMessage]\n",
    "    intent: str  # \"conversational\" or \"question\"\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 새 노드/Chain: 입력 의도 분류\n",
    "# --------------------------------------\n",
    "class ClassifyIntent(BaseModel):\n",
    "    \"\"\"\"conversational\" 또는 \"question\"으로 사용자 입력의 의도를 분류합니다.\"\"\"\n",
    "    intent: str = Field(description=\"사용자 입력의 의도. 'conversational' 또는 'question' 중 하나여야 합니다.\")\n",
    "\n",
    "classify_system_prompt = \"\"\"You are a router that classifies the user's input intent. Based on the user's latest message and the previous conversation history, determine if the input is a simple conversation/chit-chat or a question that requires information.\n",
    "- General greetings like \"Hello\", \"Thank you\", \"Have a nice day\" are 'conversational'.\n",
    "- Responses to previous answers (e.g., \"That's interesting\", \"I see\") are also 'conversational'.\n",
    "- If the input requires finding information from a PDF document or the web, it is a 'question'.\n",
    "- If in doubt, classify it as 'question'.\"\"\"\n",
    "\n",
    "classify_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", classify_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_classifier = llm.with_structured_output(ClassifyIntent)\n",
    "intent_classifier = classify_prompt | structured_llm_classifier\n",
    "\n",
    "\n",
    "def node_classify_input(state: GraphState) -> GraphState:\n",
    "    \"\"\"사용자 입력의 의도를 분류하여 state에 저장\"\"\"\n",
    "    log_debug(\"---CLASSIFYING INPUT INTENT---\")\n",
    "    intent_result = intent_classifier.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    log_debug(f\"  [Intent] Classified as: {intent_result.intent}\")\n",
    "    return {\"intent\": intent_result.intent}\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 새 노드/Chain: 단순 대화형 답변 생성\n",
    "# --------------------------------------\n",
    "conv_gen_system_prompt = \"\"\"You are a friendly AI assistant. Respond to the user's message in a natural, conversational way. Do not search for information; just generate a simple response that fits the context of the conversation.\"\"\"\n",
    "\n",
    "conv_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", conv_gen_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "conversational_chain = conv_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def node_generate_conversational_response(state: GraphState) -> GraphState:\n",
    "    \"\"\"단순 대화형 답변을 생성\"\"\"\n",
    "    log_debug(\"---GENERATING CONVERSATIONAL RESPONSE---\")\n",
    "    generation = conversational_chain.invoke({\n",
    "        \"input\": state[\"question\"],\n",
    "        \"chat_history\": state.get(\"chat_history\", [])\n",
    "    })\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# LangGraph 노드 함수\n",
    "# --------------------------\n",
    "def node_retrieve(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # 원 질문 + 히스토리 출력\n",
    "    log_debug(f\"[DEBUG] Raw Question: {question}\")\n",
    "    if chat_history:\n",
    "        log_debug(f\"[DEBUG] Chat History Count: {len(chat_history)}\")\n",
    "    else:\n",
    "        log_debug(\"[DEBUG] No chat history provided.\")\n",
    "\n",
    "    # Child 검색 결과 확인\n",
    "    child_results = vectorstore.similarity_search(question, k=2)\n",
    "    log_debug(\"=== Child 검색 결과 ===\")\n",
    "    for i, d in enumerate(child_results, 1):\n",
    "        log_debug(f\"[Child {i}] Parent ID: {d.metadata.get('doc_id')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:200]}...\\n\")\n",
    "\n",
    "    # Parent 복구 결과 (History-aware retriever 사용)\n",
    "    docs = history_aware_retriever.invoke(\n",
    "        {\"input\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    log_debug(\"=== Parent 복구 결과 ===\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        log_debug(f\"[Parent {i}] Source: {d.metadata.get('source', 'N/A')}\")\n",
    "        log_debug(f\"Snippet: {d.page_content[:500]}...\\n\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": docs,\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"web_search\": \"No\",\n",
    "        \"generation\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs: List[Document] = []\n",
    "    for d in documents:\n",
    "        try:\n",
    "            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "            if score.binary_score.strip().lower() == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        except Exception:\n",
    "            # 그레이더 실패 시 일단 보수적으로 유지\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "    web_search_flag = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search_flag,\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    return \"notify_user\" if state[\"web_search\"] == \"Yes\" else \"generate\"\n",
    "\n",
    "def node_transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    better_question = question_rewriter.invoke({\"question\": state[\"question\"]})\n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": better_question,\n",
    "        \"web_search\": state[\"web_search\"],\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "def node_web_search(state: GraphState) -> GraphState:\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    if web_search_tool is None:\n",
    "        web_results_text = \"웹검색 API 키가 설정되지 않아 웹검색을 수행하지 못했습니다.\"\n",
    "    else:\n",
    "        try:\n",
    "            results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "            # ✅ 결과가 문자열일 때 대비\n",
    "            if isinstance(results, str):\n",
    "                web_results_text = results\n",
    "            elif isinstance(results, list):\n",
    "                lines = []\n",
    "                for r in results:\n",
    "                    if isinstance(r, dict):  # dict 타입만 처리\n",
    "                        title = r.get(\"title\", \"\")\n",
    "                        url = r.get(\"url\", \"\")\n",
    "                        content = r.get(\"content\", \"\")\n",
    "                        lines.append(f\"[{title}] {url}\\n{content}\\n\")\n",
    "                    else:\n",
    "                        lines.append(str(r))  # dict가 아니면 문자열 변환\n",
    "                web_results_text = \"\\n---\\n\".join(lines) if lines else \"검색결과가 비어 있습니다.\"\n",
    "            else:\n",
    "                web_results_text = str(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            web_results_text = f\"웹검색 중 오류: {e}\"\n",
    "\n",
    "    documents = documents + [Document(page_content=web_results_text, metadata={\"source\": \"tavily\"})]\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": state[\"chat_history\"],\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def node_generate(state: GraphState) -> GraphState:\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state[\"chat_history\"]\n",
    "\n",
    "    # 답변 생성\n",
    "    answer = question_answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"context\": documents\n",
    "    })\n",
    "\n",
    "    # 출처 구분\n",
    "    if any(d.metadata.get(\"source\") == \"tavily\" for d in documents):\n",
    "        source_tag = \"\\n\\n[출처: 웹검색 결과]\"\n",
    "    else:\n",
    "        source_tag = \"\\n\\n[출처: PDF 문서]\"\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"web_search\": \"No\",\n",
    "        \"chat_history\": chat_history,\n",
    "        \"generation\": (answer if isinstance(answer, str) else str(answer)) + source_tag,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 새 노드: 사용자 알림\n",
    "# --------------------------\n",
    "def node_notify_user(state: GraphState) -> GraphState:\n",
    "    log_debug(\"---NOTIFY USER---\")\n",
    "    notice = \"문서에서 답변을 찾지 못했습니다. 인터넷 검색을 시도합니다.\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    chat_history.append(AIMessage(content=notice))  # ✅ dict로만 유지\n",
    "    return {**state, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 그래프 구성 & 컴파일\n",
    "# --------------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 1. 새 노드 등록\n",
    "workflow.add_node(\"classify_input\", node_classify_input)\n",
    "workflow.add_node(\"generate_conversational_response\", node_generate_conversational_response)\n",
    "\n",
    "# 2. 기존 노드 등록\n",
    "workflow.add_node(\"retrieve\", node_retrieve)\n",
    "workflow.add_node(\"grade_documents\", node_grade_documents)\n",
    "workflow.add_node(\"generate\", node_generate)\n",
    "workflow.add_node(\"notify_user\", node_notify_user)\n",
    "workflow.add_node(\"transform_query\", node_transform_query)\n",
    "workflow.add_node(\"web_search_node\", node_web_search)\n",
    "\n",
    "# 3. 시작점 변경\n",
    "workflow.add_edge(START, \"classify_input\")\n",
    "\n",
    "# 4. 의도에 따른 조건부 분기\n",
    "def decide_flow(state: GraphState) -> str:\n",
    "    log_debug(f\"---DECIDING FLOW BASED ON INTENT: {state['intent']}---\")\n",
    "    if state[\"intent\"] == \"conversational\":\n",
    "        return \"generate_conversational_response\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_flow,\n",
    "    {\n",
    "        \"generate_conversational_response\": \"generate_conversational_response\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 5. 기존 RAG/CRAG 흐름 연결\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    node_decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"notify_user\": \"notify_user\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"notify_user\", \"transform_query\")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "\n",
    "# 6. 종료점 연결\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"generate_conversational_response\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# run_crag 수정\n",
    "# --------------------------\n",
    "def run_crag(query: str, history: List[dict], show_debug: bool):\n",
    "    global debug_logs\n",
    "    debug_logs = []  # 실행할 때마다 초기화\n",
    "\n",
    "    chat_history_for_chain = to_lc_messages(history or [])\n",
    "    try:\n",
    "        final_state = None\n",
    "        inputs = {\"question\": query, \"chat_history\": chat_history_for_chain,\n",
    "                  \"documents\": [], \"web_search\": \"No\", \"generation\": \"\"}\n",
    "        for step in app.stream(inputs):\n",
    "            for node_name, node_state in step.items():\n",
    "                log_debug(f\"[TRACE] Node '{node_name}' passed.\")\n",
    "            final_state = node_state\n",
    "\n",
    "        # 최종 응답\n",
    "        answer = final_state.get(\"generation\", \"제공된 문서의 내용으로는 답변할 수 없습니다.\")\n",
    "        docs: List[Document] = final_state.get(\"documents\", [])\n",
    "        context_md = \"## 참조 문서\\n\\n\"\n",
    "        if docs:\n",
    "            for i, d in enumerate(docs, 1):\n",
    "                src = d.metadata.get(\"source\", \"N/A\")\n",
    "                snippet = d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
    "                context_md += f\"### 문서 {i} (source: {src})\\n```\\n{snippet}\\n```\\n\\n\"\n",
    "        else:\n",
    "            context_md += \"참조된 문서가 없습니다.\"\n",
    "\n",
    "        # 히스토리 추가 (수정된 로직)\n",
    "        # 그래프 실행 후의 최종 대화 기록을 가져옴 (여기엔 notify 메시지 등이 포함될 수 있음)\n",
    "        final_lc_history = final_state.get(\"chat_history\", chat_history_for_chain)\n",
    "        history = to_gradio_history(final_lc_history)\n",
    "\n",
    "        # 현재 사용자의 질문과 최종 답변을 히스토리에 추가\n",
    "        history.append({\"role\": \"user\", \"content\": query})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        # 디버그 표시 여부 결정\n",
    "        debug_output = \"### Debug Logs\\n```\\n\" + \"\\n\".join(debug_logs) + \"\\n```\" if show_debug else \"\"\n",
    "        return \"\", history, context_md, debug_output\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"오류 발생: {e}\\n{traceback.format_exc()}\"\n",
    "        debug_output = \"### 오류\\n```\\n\" + err + \"\\n```\"\n",
    "        return \"\", history, \"참조된 문서가 없습니다.\", debug_output\n",
    "\n",
    "\n",
    "\n",
    "def force_reload_vectorstore():\n",
    "    try:\n",
    "        print(\"[INFO] Resetting Chroma client...\")\n",
    "        vectorstore._client.reset()  # 전체 컬렉션 초기화\n",
    "        load_and_populate_vectorstore()\n",
    "        return \"✅ Vector store reloaded successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during vector store reload: {e}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 초기 적재\n",
    "# --------------------------\n",
    "load_and_populate_vectorstore()\n",
    "\n",
    "# --------------------------\n",
    "# Gradio UI\n",
    "# --------------------------\n",
    "example_questions = [\n",
    "    \"Gemini 2.5 Pro는 Gemini 1.5 Pro와 비교했을 때 어떤 점에서 향상되었나요?\",\n",
    "    \"Gemini 2.5 Pro와 Flash는 어떤 종류의 데이터를 처리할 수 있나요?\",\n",
    "    \"Gemini 2.5 시리즈의 작은 모델들은 어떤 방식으로 성능을 개선했나요?\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"PDF RAG + CRAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# PDF RAG + CRAG Chatbot (LlamaParse / ParentRetriever / History-Aware / Web Search)\")\n",
    "    gr.Markdown(\"PDF 문서 내용에 대해 질문하세요. 문서에서 못 찾으면 질문 재작성 + (선택)웹검색으로 보강합니다.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ------------------------------\n",
    "        # 왼쪽: 채팅 영역\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(height=420, label=\"Chat\", type=\"messages\", value=[])\n",
    "            msg = gr.Textbox(label=\"질문을 입력하세요... (Shift+Enter 줄바꿈)\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=example_questions,\n",
    "                inputs=msg,\n",
    "                label=\"예시 질문\"\n",
    "            )\n",
    "\n",
    "        # ------------------------------\n",
    "        # 오른쪽: 문서/옵션/디버그 영역\n",
    "        # ------------------------------\n",
    "        with gr.Column(scale=2):\n",
    "            context_display = gr.Markdown(label=\"LLM 참조 문서 전문/요약\")\n",
    "\n",
    "            with gr.Accordion(\"⚙️ Advanced Options\", open=False):\n",
    "                show_debug_checkbox = gr.Checkbox(label=\"Show Debug Logs\", value=False)\n",
    "                debug_panel = gr.Markdown(label=\"Debug Logs\")   # ✅ 디버그 로그 출력 패널\n",
    "                reload_button = gr.Button(\"🔄 Force Reload Vector Store\")\n",
    "                reload_status = gr.Markdown()\n",
    "\n",
    "    # ------------------------------\n",
    "    # 버튼/이벤트 바인딩\n",
    "    # ------------------------------\n",
    "    clear = gr.ClearButton([msg, chatbot, context_display, debug_panel])\n",
    "    msg.submit(run_crag, [msg, chatbot, show_debug_checkbox],\n",
    "               [msg, chatbot, context_display, debug_panel])\n",
    "    reload_button.click(force_reload_vectorstore, outputs=reload_status)\n",
    "\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461fbc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래프 시각화 실패 (추가 종속성 필요): Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "ASCII로 그래프 표시:\n",
      "ASCII 표시도 실패: Install grandalf to draw graphs: `pip install grandalf`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3c2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (9.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from IPython) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from stack_data->IPython) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9900d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(\"x\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f45094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement gooogle (from versions: none)\n",
      "ERROR: No matching distribution found for gooogle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.genai\n",
      "  Downloading google_genai-1.38.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google.genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google.genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google.genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google.genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google.genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google.genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google.genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google.genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sba\\miniconda3\\envs\\mp\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google.genai) (0.6.1)\n",
      "Downloading google_genai-1.38.0-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: google.genai\n",
      "Successfully installed google.genai-1.38.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gooogle\n",
    "!pip install google.genai\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
